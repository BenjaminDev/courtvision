{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to CourtVision","text":"<p>A library and simple application aims to extract useful infromation from videos of court-based1 sports. By detecting and tracking players as well as the ball CourtVision captures data at 30 frames per second. By visualizing this high resolution data it is hoped players and coaches will find interesting take-a-ways.</p> <p>An associated paper with research goals and results is available here</p>"},{"location":"#cli","title":"CLI","text":"<p>Some commonly used scripts are migrated to a cli so they are available when installation is done via <code>pip</code></p> <ul> <li><code>cv-grab-frames</code> - grabs frames from the dataset specified in <code>.env</code>.</li> </ul> <ol> <li> <p>Currently only padel is supported.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/config/","title":"courtvision.config","text":""},{"location":"api/config/#courtvision.config.CourtVisionInferenceSettings","title":"<code>CourtVisionInferenceSettings</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>All the settings for running inference and tracking.</p> <p>Note</p> <p>The settings are loaded from the .env file in the root of the project.</p> Source code in <code>courtvision/config.py</code> <pre><code>class CourtVisionInferenceSettings(BaseSettings):\n\"\"\"\n    All the settings for running inference and tracking.\n\n    !!! note\n        The settings are loaded from the .env file in the root of the project.\n    \"\"\"\n\n    ball_tracker_num_particles: int = 10_000\n\n    # Models\n    ball_detection_model_path: Path\n    player_detection_model_path: Path\n    # Calibrations\n    camera_info_path: Path\n    # Visualizations\n    court_mesh_path: Path\n    # Data\n    annotation_path: Path\n\n    class Config:\n        extra = \"ignore\"\n        env_prefix = \"COURTVISION_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n</code></pre>"},{"location":"api/config/#courtvision.config.CourtVisionTrainingSettings","title":"<code>CourtVisionTrainingSettings</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>All the settings for training a model</p> <p>Note</p> <p>The settings are loaded from the .env file in the root of the project.</p> Source code in <code>courtvision/config.py</code> <pre><code>class CourtVisionTrainingSettings(BaseSettings):\n\"\"\"\n    All the settings for training a model\n    !!! note\n        The settings are loaded from the .env file in the root of the project.\n    \"\"\"\n\n    datasets_path: Path\n    ball_models_dir: Path\n    ball_model_name: str\n    ball_checkpoints_dir: Path\n    ball_checkpoints_weights_only: Path\n\n    wb_project: str\n    wb_save_dir: str\n\n    class Config:\n        extra = \"ignore\"\n        env_prefix = \"COURTVISION_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n</code></pre>"},{"location":"api/console/","title":"courtvision.console","text":""},{"location":"api/console/#courtvision.console.grab_frames_from_clips","title":"<code>grab_frames_from_clips(frame_interval=6, max_num_frames=800, output_dir=Path('/Users/benjamindecharmoy/projects/courtvision/balldataset'))</code>","text":"<p>Grabs frames from the clips in the dataset and saves every <code>frame_interval</code>th frame to <code>output_dir</code> for a maximum of <code>max_num_frames</code> frames.</p> <p>Parameters:</p> Name Type Description Default <code>frame_interval</code> <code>int</code> <p>Save every <code>frame_interval</code>. Defaults to 6.</p> <code>6</code> <code>max_num_frames</code> <code>int</code> <p>Max number of frames to save. Defaults to 800.</p> <code>800</code> <code>output_dir</code> <code>Path</code> <p>Frames are saved to this directory. Defaults to Path( \"/Users/benjamindecharmoy/projects/courtvision/balldataset\" ).</p> <code>Path('/Users/benjamindecharmoy/projects/courtvision/balldataset')</code> Source code in <code>courtvision/console.py</code> <pre><code>def grab_frames_from_clips(\n    frame_interval: int = 6,\n    max_num_frames: int = 800,\n    output_dir: Path = Path(\n        \"/Users/benjamindecharmoy/projects/courtvision/balldataset\"\n    ),\n):\n\"\"\"Grabs frames from the clips in the dataset and saves every `frame_interval`th\n    frame to `output_dir` for a maximum of `max_num_frames` frames.\n\n    Args:\n        frame_interval (int, optional): Save every `frame_interval`. Defaults to 6.\n        max_num_frames (int, optional): Max number of frames to save. Defaults to 800.\n        output_dir (Path, optional): Frames are saved to this directory. Defaults to Path( \"/Users/benjamindecharmoy/projects/courtvision/balldataset\" ).\n    \"\"\"\n    # TODO: Add these as cli args https://github.com/BenjaminDev/courtvision/issues/10\n    ANNOTATION_PATH = Path(\n        \"/Users/benjamindecharmoy/projects/courtvision/datasets/clip_segmentations\"\n    )\n    ANNOTATION_DATA_PATH = Path(\n        \"/Users/benjamindecharmoy/projects/courtvision/datasets/clip_segmentations/data\"\n    )\n    ANNOTATION_DATA_PATH.mkdir(exist_ok=True, parents=True)\n\n    court_mesh_path = Path(\n        \"/Users/benjamindecharmoy/projects/courtvision/blender/basic_image.glb\"\n    )\n    output_dir.mkdir(exist_ok=True, parents=True)\n    annotations_file = get_latest_file(ANNOTATION_PATH, \"json\")\n    with open(annotations_file, \"r\") as f:\n        dataset = PadelDataset(samples=json.load(f))\n    print(f\"Found {len(dataset.samples)} clips\")\n    for i, (frame, uid, match_id) in enumerate(\n        frames_from_clip_segments(\n            dataset,\n            local_path=ANNOTATION_DATA_PATH,\n            stream_type=StreamType.VIDEO,\n        )\n    ):\n\n        if i % frame_interval == 0:\n            print(f\"Processing frame {i} of clip {uid}\")\n            cv2.imwrite(\n                (output_dir / f\"{uid}_{i:04}.png\").as_posix(),\n                frame[\"data\"].permute(1, 2, 0).numpy()[:, :, ::-1],\n            )\n        if i &gt;= max_num_frames:\n            break\n</code></pre>"},{"location":"api/data/","title":"courtvision.data","text":""},{"location":"api/data/#courtvision.data.AnnotationDataPath","title":"<code>AnnotationDataPath</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Tracks the location of the data for a single data item</p> Source code in <code>courtvision/data.py</code> <pre><code>class AnnotationDataPath(BaseModel):\n\"\"\"Tracks the location of the data for a single data item\"\"\"\n\n    video_url: Optional[Path] = None\n    video_local_path: Optional[Path] = None\n    image_local_path: Optional[Path] = None\n    image: Optional[Path] = Field(None, validation_alias=AliasChoices(\"img\", \"image\"))\n    # TODO: #1 use aliases to have a single source of truth file locations\n    #       both locally and on reomote - eg: s3\n\n    class Config:\n        allow_population_by_field_name = True\n</code></pre>"},{"location":"api/data/#courtvision.data.CameraInfo","title":"<code>CameraInfo</code>  <code>dataclass</code>","text":"<p>Camera calibration information</p> Source code in <code>courtvision/data.py</code> <pre><code>@dataclass\nclass CameraInfo:\n\"\"\"Camera calibration information\"\"\"\n\n    valid_for_clip_ids: set[str]\n    camera_matrix: np.array\n    distortion_coefficients: np.array\n    rotation_vector: np.array\n    translation_vector: np.array\n    image_width: int\n    image_height: int\n    error_in_reprojected_planar_points: float\n    error_in_reprojected_points: float\n\n    def world_space_to_camera_space(self) -&gt; torch.Tensor:\n        rotation_matrix, _ = cv2.Rodrigues(self.rotation_vector)\n        return torch.tensor(\n            np.vstack(\n                [\n                    np.hstack((rotation_matrix, self.translation_vector)),\n                    np.array([0, 0, 0, 1]),\n                ]\n            )\n        )\n\n    def save(self, file_name: Path):\n\"\"\"Saves the camera calibration information to a file\n\n        Args:\n            file_name (Path): The file to save the camera calibration information to.\n        \"\"\"\n        np.savez(\n            file_name,\n            camera_matrix=self.camera_matrix,\n            distortion_coefficients=self.distortion_coefficients,\n            rotation_vector=self.rotation_vector,\n            translation_vector=self.translation_vector,\n            image_width=self.image_width,\n            image_height=self.image_height,\n            error_in_reprojected_planar_points=self.error_in_reprojected_planar_points,\n            error_in_reprojected_points=self.error_in_reprojected_points,\n            valid_for_clip_ids=self.valid_for_clip_ids,\n        )\n\n    @staticmethod\n    def load(file_name: str) -&gt; Self:\n\"\"\"Loads the camera calibration information from a file.\n\n        Args:\n            file_name (str): Full path to .npz file\n\n        Returns:\n            CameraInfo: Camera calibration information\n        \"\"\"\n        data = np.load(file_name, allow_pickle=True)\n        return CameraInfo(\n            camera_matrix=data[\"camera_matrix\"],\n            distortion_coefficients=data[\"distortion_coefficients\"],\n            rotation_vector=data[\"rotation_vector\"],\n            translation_vector=data[\"translation_vector\"],\n            image_width=data[\"image_width\"],\n            image_height=data[\"image_height\"],\n            error_in_reprojected_planar_points=data[\n                \"error_in_reprojected_planar_points\"\n            ],\n            error_in_reprojected_points=data[\"error_in_reprojected_points\"],\n            valid_for_clip_ids=data[\"valid_for_clip_ids\"].tolist(),\n        )\n</code></pre>"},{"location":"api/data/#courtvision.data.CameraInfo.load","title":"<code>load(file_name)</code>  <code>staticmethod</code>","text":"<p>Loads the camera calibration information from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Full path to .npz file</p> required <p>Returns:</p> Name Type Description <code>CameraInfo</code> <code>Self</code> <p>Camera calibration information</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef load(file_name: str) -&gt; Self:\n\"\"\"Loads the camera calibration information from a file.\n\n    Args:\n        file_name (str): Full path to .npz file\n\n    Returns:\n        CameraInfo: Camera calibration information\n    \"\"\"\n    data = np.load(file_name, allow_pickle=True)\n    return CameraInfo(\n        camera_matrix=data[\"camera_matrix\"],\n        distortion_coefficients=data[\"distortion_coefficients\"],\n        rotation_vector=data[\"rotation_vector\"],\n        translation_vector=data[\"translation_vector\"],\n        image_width=data[\"image_width\"],\n        image_height=data[\"image_height\"],\n        error_in_reprojected_planar_points=data[\n            \"error_in_reprojected_planar_points\"\n        ],\n        error_in_reprojected_points=data[\"error_in_reprojected_points\"],\n        valid_for_clip_ids=data[\"valid_for_clip_ids\"].tolist(),\n    )\n</code></pre>"},{"location":"api/data/#courtvision.data.CameraInfo.save","title":"<code>save(file_name)</code>","text":"<p>Saves the camera calibration information to a file</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>Path</code> <p>The file to save the camera calibration information to.</p> required Source code in <code>courtvision/data.py</code> <pre><code>def save(self, file_name: Path):\n\"\"\"Saves the camera calibration information to a file\n\n    Args:\n        file_name (Path): The file to save the camera calibration information to.\n    \"\"\"\n    np.savez(\n        file_name,\n        camera_matrix=self.camera_matrix,\n        distortion_coefficients=self.distortion_coefficients,\n        rotation_vector=self.rotation_vector,\n        translation_vector=self.translation_vector,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        error_in_reprojected_planar_points=self.error_in_reprojected_planar_points,\n        error_in_reprojected_points=self.error_in_reprojected_points,\n        valid_for_clip_ids=self.valid_for_clip_ids,\n    )\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionArtifacts","title":"<code>CourtVisionArtifacts</code>  <code>dataclass</code>","text":"<p>Tracks the artifacts used in the pipeline</p> Source code in <code>courtvision/data.py</code> <pre><code>@dataclass\nclass CourtVisionArtifacts:\n\"\"\"Tracks the artifacts used in the pipeline\"\"\"\n\n    local_cache_path: Path\n    dataset: PadelDataset\n    ball_detector: BallDetector\n    ball_tracker: ParticleFilter\n    player_detector: PlayerDetector\n\n    court_layout: PadelCourt\n    camera_info_path: Path\n    _camera_info: CameraInfo = field(init=False, default=None)\n\n    @property\n    def camera_info(self):\n        if self._camera_info is None and self.camera_info_path.exists():\n\n            self._camera_info = CameraInfo.load(\n                self.local_cache_path / \"camera_info.npz\"\n            )\n\n        return self._camera_info\n\n    @camera_info.setter\n    def camera_info(self, value):\n        self._camera_info = value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionBallDataset","title":"<code>CourtVisionBallDataset</code>","text":"<p>         Bases: <code>VisionDataset</code></p> Source code in <code>courtvision/data.py</code> <pre><code>class CourtVisionBallDataset(VisionDataset):\n    def __init__(\n        self,\n        dataset: PadelDataset,\n        root: str,\n        download: bool = True,\n        show: Callable | None = None,\n        load_from_disk: Callable[[Path], torch.Tensor] | None = None,\n        transforms: Callable | None = None,\n        transform: Callable | None = None,\n        target_transform: Callable | None = None,\n    ):\n        super().__init__(root, transforms, transform, target_transform)\n        # self.root = root  # TODO: See what base class does and if we can use it\n        self.dataset = dataset\n        from rich.progress import track\n\n        if download:\n            for sample in track(dataset.samples, description=f\"Downloading data\"):\n                sample.data.image_local_path = download_data_item(\n                    s3_uri=sample.data.image,\n                    local_path=self.dataset.local_data_dir\n                    / sample.data.image.parent.name\n                    / sample.data.image.name,\n                )\n\n    def __len__(self):\n        return len(self.dataset.samples)\n\n    def __getitem__(self, idx) -&gt; tuple[CourtAnnotatedSample, torch.Tensor]:\n        from courtvision.vis import load_timg\n\n        # TODO: Data module should have IO functions injected into it\n        sample = self.dataset.samples[idx]\n        image = load_timg(sample.data.image_local_path)\n        return (\n            image,\n            sample,\n        )\n\n    @staticmethod\n    def collate_fn(batch):\n\"\"\"Collate function for the dataloader\"\"\"\n        images, samples = zip(*batch)\n\n        targets = [\n            {\n                \"boxes\": annotations_to_bbox(sample.annotations),\n                \"labels\": annotations_to_label(sample.annotations),\n                \"file_location\": sample.data.image_local_path.as_posix(),\n            }\n            for sample in samples\n        ]\n\n        return [o.squeeze(0) for o in images], targets\n\n    @staticmethod\n    def find_image_path(root: Path | str, sample: CourtAnnotatedSample):\n\"\"\"Finds the image path from a sample\"\"\"\n        server_file_path = Path(*sample.data.image.parts[2:])  # remove /data/\n        filename = Path(f\"{root}/{server_file_path}\")\n        return filename\n\n    @staticmethod\n    def show_sample(annotation: list[Annotation], image: torch.Tensor):\n\"\"\"Plots an image and its annotation\"\"\"\n        # TODO: Data module should have vis functions injected into it\n        from courtvision.vis import draw_rect\n\n        def draw_annotaion(annotation: Annotation, image: torch.Tensor):\n            bboxes = [\n                r.value for r in annotation.result if isinstance(r.value, RectValue)\n            ]\n\n            original_sizes = [\n                (r.original_width, r.original_height)\n                for r in annotation.result\n                if isinstance(r.value, RectValue)\n            ]\n            if bboxes:\n                rects = torch.stack(\n                    [\n                        torch.tensor(\n                            [\n                                (bbox.x / 100.0) * w_h[0],\n                                (bbox.y / 100.0) * w_h[1],\n                                (bbox.x + bbox.width) / 100.0 * w_h[0],\n                                (bbox.y + bbox.height) / 100.0 * w_h[1],\n                            ]\n                        ).unsqueeze(0)\n                        for bbox, w_h in zip(bboxes, original_sizes)\n                    ]\n                ).permute(1, 0, 2)\n                print(rects.shape)\n                draw_rect(image, bboxes=rects)\n\n            keypoints = [\n                r.value for r in annotation.result if isinstance(r.value, KeypointValue)\n            ]\n            original_sizes = [\n                (r.original_width, r.original_height)\n                for r in annotation.result\n                if isinstance(r.value, KeypointValue)\n            ]\n            if keypoints:\n                point_width = 1.0\n                rects = torch.stack(\n                    [\n                        torch.tensor(\n                            [\n                                (point.x / 100.0) * w_h[0],\n                                (point.y / 100.0) * w_h[1],\n                                (point.x + point_width) / 100.0 * w_h[0],\n                                (point.y + point_width) / 100.0 * w_h[1],\n                            ]\n                        ).unsqueeze(0)\n                        for point, w_h in zip(keypoints, original_sizes)\n                    ]\n                ).permute(1, 0, 2)\n\n                draw_rect(image, bboxes=rects)\n\n        for annot in annotation:\n            draw_annotaion(annot, image)\n        plt.imshow(image.squeeze(0).permute(1, 2, 0))\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionBallDataset.collate_fn","title":"<code>collate_fn(batch)</code>  <code>staticmethod</code>","text":"<p>Collate function for the dataloader</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef collate_fn(batch):\n\"\"\"Collate function for the dataloader\"\"\"\n    images, samples = zip(*batch)\n\n    targets = [\n        {\n            \"boxes\": annotations_to_bbox(sample.annotations),\n            \"labels\": annotations_to_label(sample.annotations),\n            \"file_location\": sample.data.image_local_path.as_posix(),\n        }\n        for sample in samples\n    ]\n\n    return [o.squeeze(0) for o in images], targets\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionBallDataset.find_image_path","title":"<code>find_image_path(root, sample)</code>  <code>staticmethod</code>","text":"<p>Finds the image path from a sample</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef find_image_path(root: Path | str, sample: CourtAnnotatedSample):\n\"\"\"Finds the image path from a sample\"\"\"\n    server_file_path = Path(*sample.data.image.parts[2:])  # remove /data/\n    filename = Path(f\"{root}/{server_file_path}\")\n    return filename\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionBallDataset.show_sample","title":"<code>show_sample(annotation, image)</code>  <code>staticmethod</code>","text":"<p>Plots an image and its annotation</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef show_sample(annotation: list[Annotation], image: torch.Tensor):\n\"\"\"Plots an image and its annotation\"\"\"\n    # TODO: Data module should have vis functions injected into it\n    from courtvision.vis import draw_rect\n\n    def draw_annotaion(annotation: Annotation, image: torch.Tensor):\n        bboxes = [\n            r.value for r in annotation.result if isinstance(r.value, RectValue)\n        ]\n\n        original_sizes = [\n            (r.original_width, r.original_height)\n            for r in annotation.result\n            if isinstance(r.value, RectValue)\n        ]\n        if bboxes:\n            rects = torch.stack(\n                [\n                    torch.tensor(\n                        [\n                            (bbox.x / 100.0) * w_h[0],\n                            (bbox.y / 100.0) * w_h[1],\n                            (bbox.x + bbox.width) / 100.0 * w_h[0],\n                            (bbox.y + bbox.height) / 100.0 * w_h[1],\n                        ]\n                    ).unsqueeze(0)\n                    for bbox, w_h in zip(bboxes, original_sizes)\n                ]\n            ).permute(1, 0, 2)\n            print(rects.shape)\n            draw_rect(image, bboxes=rects)\n\n        keypoints = [\n            r.value for r in annotation.result if isinstance(r.value, KeypointValue)\n        ]\n        original_sizes = [\n            (r.original_width, r.original_height)\n            for r in annotation.result\n            if isinstance(r.value, KeypointValue)\n        ]\n        if keypoints:\n            point_width = 1.0\n            rects = torch.stack(\n                [\n                    torch.tensor(\n                        [\n                            (point.x / 100.0) * w_h[0],\n                            (point.y / 100.0) * w_h[1],\n                            (point.x + point_width) / 100.0 * w_h[0],\n                            (point.y + point_width) / 100.0 * w_h[1],\n                        ]\n                    ).unsqueeze(0)\n                    for point, w_h in zip(keypoints, original_sizes)\n                ]\n            ).permute(1, 0, 2)\n\n            draw_rect(image, bboxes=rects)\n\n    for annot in annotation:\n        draw_annotaion(annot, image)\n    plt.imshow(image.squeeze(0).permute(1, 2, 0))\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionDataset","title":"<code>CourtVisionDataset</code>","text":"<p>         Bases: <code>VisionDataset</code></p> Source code in <code>courtvision/data.py</code> <pre><code>class CourtVisionDataset(VisionDataset):\n    def __init__(\n        self,\n        dataset: PadelDataset,\n        root: str,\n        transforms: Callable | None = None,\n        transform: Callable | None = None,\n        target_transform: Callable | None = None,\n    ):\n        self.root = root  # TODO: See what base class does and if we can use it\n        self.dataset = dataset\n        super().__init__(root, transforms, transform, target_transform)\n\n    def __len__(self):\n        return len(self.dataset.samples)\n\n    def __getitem__(self, idx) -&gt; tuple[CourtAnnotatedSample, torch.Tensor]:\n        from courtvision.vis import load_timg\n\n        # TODO: Data module should have IO functions injected into it\n        sample = self.dataset.samples[idx]\n        image = load_timg(CourtVisionDataset.find_image_path(self.root, sample=sample))\n        return (\n            sample,\n            image,\n        )\n\n    @staticmethod\n    def collate_fn(batch):\n\"\"\"Collate function for the dataloader\"\"\"\n        samples, images = zip(*batch)\n        targets = [\n            {\n                \"boxes\": annotations_to_bbox(sample.annotations),\n                \"labels\": torch.ones(1, dtype=torch.int64),\n            }\n            for sample in samples\n        ]\n\n        return targets, [o.squeeze(0) for o in images]\n\n    @staticmethod\n    def find_image_path(root: Path | str, sample: CourtAnnotatedSample):\n        server_file_path = Path(*sample.data.image.parts[2:])  # remove /data/\n        filename = Path(f\"{root}/{server_file_path}\")\n        return filename\n\n    @staticmethod\n    def show_sample(annotation: list[Annotation], image: torch.Tensor):\n\"\"\"Plots an image and its annotation\"\"\"\n        # TODO: Data module should have vis functions injected into it\n        from courtvision.vis import draw_rect\n\n        def draw_annotaion(annotation: Annotation, image: torch.Tensor):\n            bboxes = [\n                r.value for r in annotation.result if isinstance(r.value, RectValue)\n            ]\n\n            original_sizes = [\n                (r.original_width, r.original_height)\n                for r in annotation.result\n                if isinstance(r.value, RectValue)\n            ]\n            if bboxes:\n                rects = torch.stack(\n                    [\n                        torch.tensor(\n                            [\n                                (bbox.x / 100.0) * w_h[0],\n                                (bbox.y / 100.0) * w_h[1],\n                                (bbox.x + bbox.width) / 100.0 * w_h[0],\n                                (bbox.y + bbox.height) / 100.0 * w_h[1],\n                            ]\n                        ).unsqueeze(0)\n                        for bbox, w_h in zip(bboxes, original_sizes)\n                    ]\n                ).permute(1, 0, 2)\n                print(rects.shape)\n                draw_rect(image, bboxes=rects)\n\n            keypoints = [\n                r.value for r in annotation.result if isinstance(r.value, KeypointValue)\n            ]\n            original_sizes = [\n                (r.original_width, r.original_height)\n                for r in annotation.result\n                if isinstance(r.value, KeypointValue)\n            ]\n            if keypoints:\n                point_width = 1.0\n                rects = torch.stack(\n                    [\n                        torch.tensor(\n                            [\n                                (point.x / 100.0) * w_h[0],\n                                (point.y / 100.0) * w_h[1],\n                                (point.x + point_width) / 100.0 * w_h[0],\n                                (point.y + point_width) / 100.0 * w_h[1],\n                            ]\n                        ).unsqueeze(0)\n                        for point, w_h in zip(keypoints, original_sizes)\n                    ]\n                ).permute(1, 0, 2)\n\n                draw_rect(image, bboxes=rects)\n\n        for annot in annotation:\n            draw_annotaion(annot, image)\n        plt.imshow(image.squeeze(0).permute(1, 2, 0))\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionDataset.collate_fn","title":"<code>collate_fn(batch)</code>  <code>staticmethod</code>","text":"<p>Collate function for the dataloader</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef collate_fn(batch):\n\"\"\"Collate function for the dataloader\"\"\"\n    samples, images = zip(*batch)\n    targets = [\n        {\n            \"boxes\": annotations_to_bbox(sample.annotations),\n            \"labels\": torch.ones(1, dtype=torch.int64),\n        }\n        for sample in samples\n    ]\n\n    return targets, [o.squeeze(0) for o in images]\n</code></pre>"},{"location":"api/data/#courtvision.data.CourtVisionDataset.show_sample","title":"<code>show_sample(annotation, image)</code>  <code>staticmethod</code>","text":"<p>Plots an image and its annotation</p> Source code in <code>courtvision/data.py</code> <pre><code>@staticmethod\ndef show_sample(annotation: list[Annotation], image: torch.Tensor):\n\"\"\"Plots an image and its annotation\"\"\"\n    # TODO: Data module should have vis functions injected into it\n    from courtvision.vis import draw_rect\n\n    def draw_annotaion(annotation: Annotation, image: torch.Tensor):\n        bboxes = [\n            r.value for r in annotation.result if isinstance(r.value, RectValue)\n        ]\n\n        original_sizes = [\n            (r.original_width, r.original_height)\n            for r in annotation.result\n            if isinstance(r.value, RectValue)\n        ]\n        if bboxes:\n            rects = torch.stack(\n                [\n                    torch.tensor(\n                        [\n                            (bbox.x / 100.0) * w_h[0],\n                            (bbox.y / 100.0) * w_h[1],\n                            (bbox.x + bbox.width) / 100.0 * w_h[0],\n                            (bbox.y + bbox.height) / 100.0 * w_h[1],\n                        ]\n                    ).unsqueeze(0)\n                    for bbox, w_h in zip(bboxes, original_sizes)\n                ]\n            ).permute(1, 0, 2)\n            print(rects.shape)\n            draw_rect(image, bboxes=rects)\n\n        keypoints = [\n            r.value for r in annotation.result if isinstance(r.value, KeypointValue)\n        ]\n        original_sizes = [\n            (r.original_width, r.original_height)\n            for r in annotation.result\n            if isinstance(r.value, KeypointValue)\n        ]\n        if keypoints:\n            point_width = 1.0\n            rects = torch.stack(\n                [\n                    torch.tensor(\n                        [\n                            (point.x / 100.0) * w_h[0],\n                            (point.y / 100.0) * w_h[1],\n                            (point.x + point_width) / 100.0 * w_h[0],\n                            (point.y + point_width) / 100.0 * w_h[1],\n                        ]\n                    ).unsqueeze(0)\n                    for point, w_h in zip(keypoints, original_sizes)\n                ]\n            ).permute(1, 0, 2)\n\n            draw_rect(image, bboxes=rects)\n\n    for annot in annotation:\n        draw_annotaion(annot, image)\n    plt.imshow(image.squeeze(0).permute(1, 2, 0))\n</code></pre>"},{"location":"api/data/#courtvision.data.KeypointValue","title":"<code>KeypointValue</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Specifies a keypoint and it's labels</p> Source code in <code>courtvision/data.py</code> <pre><code>class KeypointValue(BaseModel):\n\"\"\"Specifies a keypoint and it's labels\"\"\"\n\n    x: float\n    y: float\n    width: float\n    keypointlabels: list[str]\n</code></pre>"},{"location":"api/data/#courtvision.data.LabelValue","title":"<code>LabelValue</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Specifies a clip segment and it's labels</p> Source code in <code>courtvision/data.py</code> <pre><code>class LabelValue(BaseModel):\n\"\"\"Specifies a clip segment and it's labels\"\"\"\n\n    start: float\n    end: float\n    labels: list[str]\n</code></pre>"},{"location":"api/data/#courtvision.data.PadelCourt","title":"<code>PadelCourt</code>  <code>dataclass</code>","text":"<p>Padel court dimensions and locations of key points</p> Source code in <code>courtvision/data.py</code> <pre><code>@dataclass\nclass PadelCourt:\n\"\"\"Padel court dimensions and locations of key points\"\"\"\n\n    # The scale of the court is in meters\n    # Setting this to 100.0 means that the court is 1_000cm x 2_000cm\n    court_scale: float = 10.0\n\n    # REF: https://www.lta.org.uk/4ad2a4/siteassets/play/padel/file/lta-padel-court-guidance.pdf\n    width: float = 10.0 * court_scale\n    length: float = 20.0 * court_scale\n    backwall_height: float = 3.0 * court_scale\n    backwall_fence_height: float = 4.0 * court_scale\n    serve_line_from_back_line: float = 3.0 * court_scale\n    line_width: float = 0.05 * court_scale\n    net_height: float = 0.78 * court_scale  # 0.78m\n\n    @classmethod\n    @property\n    def center_line(cls) -&gt; np.array:\n        return np.array(\n            [\n                (cls.width / 2, cls.length - cls.serve_line_from_back_line),\n                (cls.width / 2, cls.serve_line_from_back_line),\n            ],\n            dtype=np.int32,\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def net_line(cls) -&gt; np.array:\n        return np.array(\n            [(0, cls.length / 2), (cls.width, cls.length / 2)], dtype=np.int64\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def near_serve_line(cls):\n        return np.array(\n            [\n                (0, cls.length - cls.serve_line_from_back_line),\n                (cls.width, cls.length - cls.serve_line_from_back_line),\n            ],\n            np.int32,\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def far_serve_line(cls):\n        return np.array(\n            [\n                (0, cls.serve_line_from_back_line),\n                (cls.width, cls.serve_line_from_back_line),\n            ],\n            dtype=np.int32,\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def front_left(cls):\n        return (0.0, 0.0)\n\n    @classmethod\n    @property\n    def front_right(cls):\n        return (cls.width, 0)\n\n    @classmethod\n    @property\n    def top_front_left_vertical_plane(cls):\n        # x, z\n        return (0.0, cls.backwall_height)\n\n    @classmethod\n    @property\n    def top_front_right_vertical_plane(cls):\n        # x, z\n        return (cls.width, cls.backwall_height)\n\n    @classmethod\n    @property\n    def back_left(cls):\n        return (0.0, cls.length)\n\n    @classmethod\n    @property\n    def back_right(cls):\n        return (cls.width, cls.length)\n\n    @classmethod\n    @property\n    def left_near_serve_line(cls):\n        return (0.0, cls.serve_line_from_back_line)\n\n    @classmethod\n    @property\n    def right_near_serve_line(cls):\n        return (cls.width, cls.serve_line_from_back_line)\n\n    @classmethod\n    @property\n    def left_far_serve_line(cls):\n        return (0.0, cls.length - cls.serve_line_from_back_line)\n\n    @classmethod\n    @property\n    def right_far_serve_line(cls):\n        return (cls.width, cls.length - cls.serve_line_from_back_line)\n\n    @classmethod\n    @property\n    def m_top_front_left(cls):\n        # TODO: add thes\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    def n_top_front_right(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    def o_top_back_left(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    def p_top_back_right(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    def q_top_net_line_left(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    def r_top_net_line_right(cls):\n        raise NotImplementedError()\n\n    # Normalised:\n    @classmethod\n    @property\n    def center_line_n(cls) -&gt; np.array:\n        return np.array(\n            [\n                ((cls.width / 2) / cls.width, cls.length / cls.length),\n                ((cls.width / 2) / cls.width, 0),\n            ],\n            dtype=np.int32,\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def net_line_n(cls) -&gt; np.array:\n        return np.array(\n            [\n                (0, (cls.length / 2) / cls.length),\n                (cls.width / cls.width, (cls.length / 2) / cls.length),\n            ],\n            dtype=np.int64,\n        ).reshape(-1, 1, 2)\n\n    @classmethod\n    @property\n    def front_left_n(cls):\n        return (cls.front_left[0] / cls.width, cls.front_left[1] / cls.length)\n\n    @classmethod\n    @property\n    def front_right_n(cls):\n        return (cls.front_right[0] / cls.width, cls.front_right[1] / cls.length)\n\n    @classmethod\n    @property\n    def top_front_left_vertical_plane_n(cls):\n        # x, z\n        return (0.0, 0.0)\n\n    @classmethod\n    @property\n    def top_front_right_vertical_plane_n(cls):\n        # x, z\n        return (cls.width / cls.width, 0.0)\n\n    @classmethod\n    @property\n    def front_left_vertical_plane_n(cls):\n        # x, z\n        return (0.0, cls.backwall_height / cls.backwall_height)\n\n    @classmethod\n    @property\n    def front_right_vertical_plane_n(cls):\n        # x, z\n        return (cls.width / cls.width, cls.backwall_height / cls.backwall_height)\n\n    @classmethod\n    @property\n    def back_left_n(cls):\n        return (cls.back_left[0] / cls.width, cls.back_left[1] / cls.length)\n\n    @classmethod\n    @property\n    def back_right_n(cls):\n        return (cls.back_right[0] / cls.width, cls.back_right[1] / cls.length)\n\n    @classmethod\n    @property\n    def left_near_serve_line_n(cls):\n        return (\n            cls.left_near_serve_line[0] / cls.width,\n            cls.left_near_serve_line[1] / cls.length,\n        )\n\n    @classmethod\n    @property\n    def right_near_serve_line_n(cls):\n        return (\n            cls.right_near_serve_line[0] / cls.width,\n            cls.right_near_serve_line[1] / cls.length,\n        )\n\n    @classmethod\n    @property\n    def left_far_serve_line_n(cls):\n        return (\n            cls.left_far_serve_line[0] / cls.width,\n            cls.left_far_serve_line[1] / cls.length,\n        )\n\n    @classmethod\n    @property\n    def right_far_serve_line_n(cls):\n        return (\n            cls.right_far_serve_line[0] / cls.width,\n            cls.right_far_serve_line[1] / cls.length,\n        )\n</code></pre>"},{"location":"api/data/#courtvision.data.RectValue","title":"<code>RectValue</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Specifies a rectangle and it's labels</p> Source code in <code>courtvision/data.py</code> <pre><code>class RectValue(BaseModel):\n\"\"\"Specifies a rectangle and it's labels\"\"\"\n\n    x: float\n    y: float\n    width: float\n    height: float\n    rectanglelabels: list[str]\n</code></pre>"},{"location":"api/data/#courtvision.data.VideoRectSequence","title":"<code>VideoRectSequence</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Specifies a rectangle and it's labels for a frames in a sequence</p> Source code in <code>courtvision/data.py</code> <pre><code>class VideoRectSequence(BaseModel):\n\"\"\"Specifies a rectangle and it's labels for a frames in a sequence\"\"\"\n\n    frame: int\n    enabled: bool\n    rotation: float\n    x: float\n    y: float\n    width: float\n    height: float\n    time: float\n</code></pre>"},{"location":"api/data/#courtvision.data.VideoRectValue","title":"<code>VideoRectValue</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Specifies a sequence of rectangles and it's labels</p> Source code in <code>courtvision/data.py</code> <pre><code>class VideoRectValue(BaseModel):\n\"\"\"Specifies a sequence of rectangles and it's labels\"\"\"\n\n    # TODO: rename to VideoRectSequenceValue https://github.com/BenjaminDev/courtvision/issues/11\n    framesCount: int\n    duration: float\n    sequence: list[VideoRectSequence]\n    labels: list[str]\n</code></pre>"},{"location":"api/data/#courtvision.data.annotations_to_bbox","title":"<code>annotations_to_bbox(annotations)</code>","text":"<p>Grab the bounding boxes from the annotations.</p> <p>Note</p> <p>Coordinates are in image coordinates and not normalised coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>list[Annotation]</code> <p>Annotations from the dataset.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: A tensor of bounding boxes in image coordinates.</p> Source code in <code>courtvision/data.py</code> <pre><code>def annotations_to_bbox(annotations: list[Annotation]) -&gt; torch.Tensor:\n\"\"\"Grab the bounding boxes from the annotations.\n    !!! note\n        Coordinates are in image coordinates and *not* normalised coordinates.\n    Args:\n        annotations (list[Annotation]): Annotations from the dataset.\n\n    Returns:\n        torch.Tensor: A tensor of bounding boxes in image coordinates.\n    \"\"\" \"\"\"\"\"\"\n    bboxes = []\n    original_sizes = []\n    for annotation in annotations:\n        bboxes.extend(\n            [r.value for r in annotation.result if isinstance(r.value, RectValue)]\n        )\n        original_sizes.extend(\n            [\n                (r.original_width, r.original_height)\n                for r in annotation.result\n                if isinstance(r.value, RectValue)\n            ]\n        )\n    return torch.stack(\n        [\n            torch.tensor(\n                [\n                    (bbox.x / 100.0) * w_h[0],\n                    (bbox.y / 100.0) * w_h[1],\n                    (bbox.x + bbox.width) / 100.0 * w_h[0],\n                    (bbox.y + bbox.height) / 100.0 * w_h[1],\n                ]\n            )\n            for bbox, w_h in zip(bboxes, original_sizes)\n        ]\n    )\n</code></pre>"},{"location":"api/data/#courtvision.data.annotations_to_label","title":"<code>annotations_to_label(annotations)</code>","text":"<p>Grab the labels from the annotations</p> <p>Note</p> <p>Currently only supports a single label and rects only!</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>list[Annotation]</code> <p>Annotations from the dataset.</p> required <p>Returns:</p> Type Description <code>torch.IntTensor</code> <p>torch.IntTensor: A tensor of labels.</p> Source code in <code>courtvision/data.py</code> <pre><code>def annotations_to_label(annotations: list[Annotation]) -&gt; torch.IntTensor:\n\"\"\"Grab the labels from the annotations\n    !!! note\n        Currently only supports a single label and rects only!\n\n    Args:\n        annotations (list[Annotation]): Annotations from the dataset.\n\n    Returns:\n        torch.IntTensor: A tensor of labels.\n    \"\"\"\n    labels = []\n    for annotation in annotations:\n        labels.extend(\n            [\n                r.value.rectanglelabels[0]\n                for r in annotation.result\n                if isinstance(r.value, RectValue)\n            ]\n        )\n\n    return torch.ones(len(labels), dtype=torch.int64)\n</code></pre>"},{"location":"api/data/#courtvision.data.collate_fn","title":"<code>collate_fn(batch)</code>","text":"<p>Collate function for the dataloader</p> Source code in <code>courtvision/data.py</code> <pre><code>def collate_fn(batch):\n\"\"\"Collate function for the dataloader\"\"\"\n    samples, images = zip(*batch)\n    targets = [\n        {\n            \"boxes\": annotations_to_bbox(sample.annotations),\n            \"labels\": torch.ones(1, dtype=torch.int64),\n        }\n        for sample in samples\n    ]\n\n    return targets, [o.squeeze(0) for o in images]\n</code></pre>"},{"location":"api/data/#courtvision.data.dict_to_points","title":"<code>dict_to_points(keypoints)</code>","text":"<p>Unpacks a dict of keypoints into a np.array of points and a list of labels</p> <p>Parameters:</p> Name Type Description Default <code>keypoints</code> <code>dict[str, tuple[float, float]]</code> <p>Dict of keypoints</p> required <p>Returns:</p> Type Description <code>tuple[np.array, list[str]]</code> <p>np.array, list[str]: Nx2 array of points and list of labels</p> Source code in <code>courtvision/data.py</code> <pre><code>def dict_to_points(\n    keypoints: dict[str, tuple[float, float]]\n) -&gt; tuple[np.array, list[str]]:\n\"\"\"Unpacks a dict of keypoints into a np.array of points and a list of labels\n\n    Args:\n        keypoints (dict[str, tuple[float, float]]): Dict of keypoints\n\n    Returns:\n        np.array, list[str]: Nx2 array of points and list of labels\n    \"\"\"\n    keypoints = dict(sorted(keypoints.items(), key=lambda x: x[0]))\n    return np.array(list(keypoints.values())).astype(np.float32), list(keypoints.keys())\n</code></pre>"},{"location":"api/data/#courtvision.data.download_data_item","title":"<code>download_data_item(s3_uri, local_path, s3_client=None, use_cached=True)</code>","text":"<p>Note</p> <p><code>courtvision-padel-dataset</code> profile must be configured in ~/.aws/credentials</p> <p>Parameters:</p> Name Type Description Default <code>s3_uri</code> <code>str</code> <p>S3 uri to file</p> required <code>local_path</code> <code>Path</code> <p>Path to file on local filesystem</p> required <code>s3_client</code> <code>_type_</code> <p>A suitable s3_client (access to s3). Defaults to None.</p> <code>None</code> <code>use_cached</code> <code>bool</code> <p>If True and the file exists uses the one on disk. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the data item on local filesystem</p> Source code in <code>courtvision/data.py</code> <pre><code>def download_data_item(\n    s3_uri: str, local_path: Path, s3_client=None, use_cached=True\n) -&gt; Path:\n\"\"\"\n\n    !!! note\n        `courtvision-padel-dataset` profile must be configured in ~/.aws/credentials\n\n    Args:\n        s3_uri (str): S3 uri to file\n        local_path (Path): Path to file on local filesystem\n        s3_client (_type_, optional): A suitable s3_client (access to s3). Defaults to None.\n        use_cached (bool, optional): If True and the file exists uses the one on disk. Defaults to True.\n\n    Returns:\n        Path: Path to the data item on local filesystem\n    \"\"\"\n    if use_cached and local_path.exists():\n        return local_path\n\n    if s3_client is None:\n        import boto3\n\n        session = boto3.Session(profile_name=\"courtvision-padel-dataset\")\n        s3_client = session.client(\"s3\", region_name=\"us-east-1\")\n    bucket_name = s3_uri.parents[-3].name\n    object_name = \"/\".join(s3_uri.parts[-3:])\n    local_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(local_path, \"wb\") as fp:\n        s3_client.download_fileobj(bucket_name, object_name, fp)\n    return local_path\n</code></pre>"},{"location":"api/data/#courtvision.data.frames_from_clip_segments","title":"<code>frames_from_clip_segments(dataset, local_path, stream_type=StreamType.VIDEO)</code>","text":"<p>Graps frames for each clip segment in the dataset. A unique id is generated for each clip segment. Frames can be either audio or video frames.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PadelDataset</code> <p>A dataset of annotated clips</p> required <code>local_path</code> <code>Path</code> <p>if the file is not already downloaded, it will be downloaded to this path</p> required <code>stream_type</code> <code>StreamType</code> <p>Either <code>StreamType.VIDEO</code> or <code>StreamType.AUDIO</code>. Defaults to StreamType.VIDEO.</p> <code>StreamType.VIDEO</code> <p>Yields:</p> Type Description <code>Tuple[dict[str, torch.Tensor], str]</code> <p><code>{\"data\": torch.Tensor, \"pts\": torch.Tensor}, unique_id, match_id</code></p> <code>Tuple[dict[str, torch.Tensor], str]</code> <p>where <code>unique_id</code> is the md5 of the annotation unique_id and the start and end times of the clip.</p> <code>Tuple[dict[str, torch.Tensor], str]</code> <p>And <code>pts</code> is a presentation timestamp of the frame expressed in seconds.</p> <code>Tuple[dict[str, torch.Tensor], str]</code> <p><code>match_id</code> is the parent folder of the clip.</p> Source code in <code>courtvision/data.py</code> <pre><code>def frames_from_clip_segments(\n    dataset: PadelDataset,\n    local_path: Path,\n    stream_type: StreamType = StreamType.VIDEO,\n) -&gt; Tuple[dict[str, torch.Tensor], str]:\n\"\"\"\n    Graps frames for each clip segment in the dataset. A unique id is generated for each clip segment.\n    Frames can be either audio or video frames.\n\n    Args:\n        dataset (PadelDataset): A dataset of annotated clips\n        local_path (Path): if the file is not already downloaded, it will be downloaded to this path\n        stream_type (StreamType, optional): Either `StreamType.VIDEO` or `StreamType.AUDIO`. Defaults to StreamType.VIDEO.\n\n    Yields:\n        `{\"data\": torch.Tensor, \"pts\": torch.Tensor}, unique_id, match_id`\n        where `unique_id` is the md5 of the annotation unique_id and the start and end times of the clip.\n        And `pts` is a presentation timestamp of the frame expressed in seconds.\n        `match_id` is the parent folder of the clip.\n    \"\"\"\n    from rich.progress import track\n\n    for sample in track(dataset.samples, description=f\"Downloading data\"):\n        # for sample in dataset.samples:\n        sample.data.video_local_path = download_data_item(\n            s3_uri=sample.data.video_url,\n            local_path=local_path\n            / sample.data.video_url.parent.name\n            / sample.data.video_url.name,\n        )\n    for sample in dataset.samples:\n        for annotation in sample.annotations:\n            for result in annotation.result:\n                if isinstance(result.value, LabelValue):\n                    start_time = result.value.start\n                    end_time = result.value.end\n                    reader = torchvision.io.VideoReader(\n                        sample.data.video_local_path.as_posix(), stream_type.value\n                    )\n                    reader.seek(start_time)\n                try:\n                    while frame := next(reader):\n                        if frame[\"pts\"] &lt; start_time:\n                            # seeks is not always accuarte!\n                            # burn frames until we get to the right time.\n                            # Alternative - build torchvision from source with video_reader backend\n                            continue\n                        if frame[\"pts\"] &gt; end_time:\n                            break\n                        yield frame, md5(\n                            f\"{start_time}{end_time}{annotation.unique_id}\".encode()\n                        ).hexdigest(), f\"{sample.data.video_local_path.parent.name}\"\n                except Exception as e:\n                    print(f\"{sample.data.video_local_path=} has invalid data {e=}\")\n                    continue\n</code></pre>"},{"location":"api/data/#courtvision.data.get_keypoints_as_dict","title":"<code>get_keypoints_as_dict(results)</code>","text":"<p>Go through the results and return a dict of keypoints</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[GeneralResult]</code> <p>List of results from the annotation</p> required <p>Returns:</p> Type Description <code>dict[str, tuple[float, float]]</code> <p>dict[str, tuple[float, float]]: keypoints in absolute coordinates eg: keypoints[\"{some keypoint}\"] = (x, y)</p> Source code in <code>courtvision/data.py</code> <pre><code>def get_keypoints_as_dict(\n    results: list[GeneralResult],\n) -&gt; dict[str, tuple[float, float]]:\n\"\"\"Go through the results and return a dict of keypoints\n\n    Args:\n        results (list[GeneralResult]): List of results from the annotation\n\n    Returns:\n        dict[str, tuple[float, float]]: keypoints in absolute coordinates eg: keypoints[\"{some keypoint}\"] = (x, y)\n    \"\"\"\n    keypoints = {}\n    for result in results:\n        if isinstance(result.value, KeypointValue):\n            keypoints[result.value.keypointlabels[0]] = (\n                result.value.x / 100.0 * result.original_width,\n                result.value.y / 100.0 * result.original_height,\n            )\n    return keypoints\n</code></pre>"},{"location":"api/data/#courtvision.data.get_normalized_calibration_image_points_and_clip_ids","title":"<code>get_normalized_calibration_image_points_and_clip_ids(dataset)</code>","text":"<p>Note</p> <p>This assumes that the calibration points are the only annotations with a VideoRectValue and the points of the same label are in the same place as the last one which will be used.</p> <p>Note</p> <p>Points are normalized to 0-1. Not -1 to 1 like in kornia.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PadelDataset</code> <p>Dataset descibing a video with calibration points.</p> required <p>Returns:</p> Name Type Description <code>image_points</code> <code>dict[str, tuple[float, float]]</code> <p>Returns a dict of image points in normalized coordinates. And</p> <code>set[str]</code> <p>the clip_ids (set[str]) that are accociated with the calibration points.</p> Source code in <code>courtvision/data.py</code> <pre><code>def get_normalized_calibration_image_points_and_clip_ids(\n    dataset: PadelDataset,\n) -&gt; tuple[dict[str, tuple[float, float]], set[str]]:\n\"\"\"\n    !!! note\n        This assumes that the calibration points are the only annotations with a VideoRectValue\n        and the points of the same label are in the same place as the last one which will be used.\n\n    !!! note\n        Points are normalized to 0-1. Not -1 to 1 like in kornia.\n    Args:\n        dataset (PadelDataset): Dataset descibing a video with calibration points.\n\n    Returns:\n        image_points (dict[str, tuple[float, float]]): Returns a dict of image points in normalized coordinates. And\n        the clip_ids (set[str]) that are accociated with the calibration points.\n    \"\"\"\n    calibration_image_points = {}\n    clip_source = set([])\n    for sample in dataset.samples:\n        if sample.data.video_url:\n            clip_source.add(sample.data.video_url)\n        if sample.data.video_local_path:\n            clip_source.add(sample.data.video_local_path)\n        if sample.data.image:\n            clip_source.add(sample.data.image)\n        if not clip_source:\n            raise ValueError(\"No clip source found\")\n        for annotation in sample.annotations:\n\n            for result in annotation.result:\n                if isinstance(result.value, VideoRectValue):\n                    for label, rect in zip(result.value.labels, result.value.sequence):\n                        calibration_image_points[label] = (\n                            (rect.x + rect.width / 2) / 100.0,\n                            (rect.y + rect.height / 2) / 100.0,\n                        )\n    return calibration_image_points, clip_source\n</code></pre>"},{"location":"api/data/#courtvision.data.validate_dataloader","title":"<code>validate_dataloader(dataloader)</code>","text":"<p>Runs over all items in a dataloader and validates the annotations.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>A dataloader with a collate_fn that returns a                     list of annotations and a list of images.</p> required Source code in <code>courtvision/data.py</code> <pre><code>def validate_dataloader(dataloader: DataLoader):\n\"\"\"Runs over all items in a dataloader and validates the annotations.\n\n    Args:\n        dataloader (DataLoader): A dataloader with a collate_fn that returns a\n                                list of annotations and a list of images.\n    \"\"\" \"\"\"\"\"\"\n    for (images, targets) in dataloader:\n        assert all(o[\"boxes\"].shape for o in targets)\n        assert all(o.shape for o in images)\n        for image, target in zip(images, targets):\n            height, width = image.shape[1:]\n            assert all(x &gt; 0 and x &lt; width for x in target[\"boxes\"][0][::2])\n            assert all(y &gt; 0 and y &lt; height for y in target[\"boxes\"][0][1::2])\n</code></pre>"},{"location":"api/geometry/","title":"courtvision.geometry","text":""},{"location":"api/geometry/#courtvision.geometry.camera_space_to_world_space","title":"<code>camera_space_to_world_space(camera_point, translation_vector, rotation_vector)</code>","text":"<p>Transform a point from camera space to world space.</p> <p>Parameters:</p> Name Type Description Default <code>camera_point</code> <code>np.array</code> <p>3D point in camera space with shape (3,)</p> required <code>translation_vector</code> <code>np.array</code> <p>Translation vector of the camera with shape (3,)</p> required <code>rotation_vector</code> <code>np.array</code> <p>Rotation vector of the camera with shape (3,)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the camera point is not a 3D point with shape (3,)</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: 3D point in world space with shape (3,)</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def camera_space_to_world_space(\n    camera_point: np.array,\n    translation_vector: np.array,\n    rotation_vector: np.array,\n) -&gt; np.array:\n\"\"\"Transform a point from camera space to world space.\n\n    Args:\n        camera_point (np.array): 3D point in camera space with shape (3,)\n        translation_vector (np.array): Translation vector of the camera with shape (3,)\n        rotation_vector (np.array): Rotation vector of the camera with shape (3,)\n\n    Raises:\n        ValueError: If the camera point is not a 3D point with shape (3,)\n\n    Returns:\n        np.array: 3D point in world space with shape (3,)\n    \"\"\"\n    if not camera_point.shape[0] == 3:\n        raise ValueError(\"Camera point must be a 3D point with shape (3,)\")\n    R, _ = cv2.Rodrigues(rotation_vector)\n    world_point = (R.T @ (camera_point - translation_vector)).T\n    return world_point\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.compute_ray_intersecting_plane","title":"<code>compute_ray_intersecting_plane(point_a_on_ray, point_b_on_ray, plane_normal=np.array([[0, 0, 1]]), plane_point=np.array([0, 0, 0]))</code>","text":"<p>Given two points on a ray, compute the point of intersection with a plane. The plane is defined as a normal vector and a point on the plane.</p> <p>Parameters:</p> Name Type Description Default <code>point_a_on_ray</code> <code>np.array</code> <p>3D point on the ray with shape (3, 1)</p> required <code>point_b_on_ray</code> <code>np.array</code> <p>3D point on the ray with shape (3, 1)</p> required <code>plane_normal</code> <code>np.array</code> <p>Unit vector pointing out the plane. Defaults to np.array([[0, 0, 1]]). Shape (1, 3)</p> <code>np.array([[0, 0, 1]])</code> <code>plane_point</code> <code>np.array</code> <p>Point on the plane. Defaults to np.array([0, 0, 0]). Shape (3,)</p> <code>np.array([0, 0, 0])</code> <p>Returns:</p> Type Description <p>np.array: Returns the 3D point of intersection with shape (3,)</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def compute_ray_intersecting_plane(\n    point_a_on_ray: np.array,\n    point_b_on_ray: np.array,\n    plane_normal: np.array = np.array([[0, 0, 1]]),\n    plane_point: np.array = np.array([0, 0, 0]),\n):\n\"\"\"Given two points on a ray, compute the point of intersection with a plane.\n    The plane is defined as a normal vector and a point on the plane.\n\n    Args:\n        point_a_on_ray (np.array): 3D point on the ray with shape (3, 1)\n        point_b_on_ray (np.array): 3D point on the ray with shape (3, 1)\n        plane_normal (np.array, optional): Unit vector pointing out the plane. Defaults to np.array([[0, 0, 1]]). Shape (1, 3)\n        plane_point (np.array, optional): Point on the plane. Defaults to np.array([0, 0, 0]). Shape (3,)\n\n    Returns:\n        np.array: Returns the 3D point of intersection with shape (3,)\n    \"\"\"\n    # Vector along the ray direction A -&gt; B\n    if not (point_a_on_ray.shape == point_b_on_ray.shape == (3, 1)):\n        raise ValueError(\n            f\"Point A and B must be 3D points with shape (3, 1). {point_a_on_ray.shape=} {point_b_on_ray.shape=}\"\n        )\n    if not plane_normal.shape == (1, 3) and np.linalg.norm(plane_normal) == 1:\n        raise ValueError(\n            \"Plane normal must be a unit vector with shape (1, 3) and norm 1\"\n        )\n    if not plane_point.shape == (3,):\n        raise ValueError(\"Plane point must be a 3D point with shape (3,)\")\n\n    ray_direction = point_b_on_ray - point_a_on_ray\n    # Finding parameter t\n    t = -(np.dot(plane_normal, point_a_on_ray) + plane_point) / np.dot(\n        plane_normal, ray_direction\n    )\n    # Finding point of intersection\n    intersection = (point_a_on_ray.T + t * ray_direction.T).squeeze(0)\n    return intersection\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.convert_corners_to_coords","title":"<code>convert_corners_to_coords(corners)</code>","text":"<p>Convert <code>corners_world_xx_n</code> to a numpy array of shape (12,)</p> <p>Parameters:</p> Name Type Description Default <code>corners</code> <code>dict</code> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: numpy array of shape (12,)</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def convert_corners_to_coords(corners: dict) -&gt; np.ndarray:\n\"\"\"Convert `corners_world_xx_n` to a numpy array of shape (12,)\n\n    Args:\n        corners (dict):\n\n    Returns:\n        np.ndarray: numpy array of shape (12,)\n    \"\"\"\n    vec_of_positions = convert_corners_to_vec(corners=corners)\n    if \"z\" in vec_of_positions:\n        return np.array(\n            [\n                (x, y, z)\n                for x, y, z in zip(\n                    vec_of_positions[\"x\"], vec_of_positions[\"y\"], vec_of_positions[\"z\"]\n                )\n            ],\n            dtype=np.float32,\n        )\n    return np.array(\n        [(x, y) for x, y in zip(vec_of_positions[\"x\"], vec_of_positions[\"y\"])],\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.convert_corners_to_vec","title":"<code>convert_corners_to_vec(corners)</code>","text":"<p>Convert <code>corners_world_xx_n</code> to a dict of vectors</p> <p>Parameters:</p> Name Type Description Default <code>corners</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict with keys x, y, z and numpy array of each</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def convert_corners_to_vec(corners: dict) -&gt; dict:\n\"\"\"Convert `corners_world_xx_n` to a dict of vectors\n\n    Args:\n        corners (dict):\n\n    Returns:\n        dict: dict with keys x, y, z and numpy array of each\n    \"\"\"\n    vec_of_positions = defaultdict(partial(np.ndarray, 0))\n    for _, x_y_z in corners.items():\n        for axis, value in zip([\"x\", \"y\", \"z\"], x_y_z, strict=False):\n            vec_of_positions[axis] = np.append(vec_of_positions[axis], value)\n    return vec_of_positions\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.convert_obj_points_to_planar","title":"<code>convert_obj_points_to_planar(object_points)</code>","text":"<p>Converts object points to planar points by finding the common axis and permuting the points so that the common axis is the last axis. Assumes that the object points are planar.</p> <p>Parameters:</p> Name Type Description Default <code>object_points</code> <code>np.array</code> <p>description</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When points are not planar</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: description</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def convert_obj_points_to_planar(object_points: np.array) -&gt; np.array:\n\"\"\"Converts object points to planar points by finding the common axis and permuting the points so that the common axis is the last axis.\n    Assumes that the object points are planar.\n\n    Args:\n        object_points (np.array): _description_\n\n    Raises:\n        ValueError: When points are not planar\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    common_axis = None\n    for axis in [0, 1, 2]:\n        if all(object_points[0, axis] == o for o in object_points[:, axis]):\n            common_axis = axis\n            break\n    if common_axis is None:\n        raise ValueError(\"Could not find common axis\")\n    # permute the object points so that the common axis is the last axis\n    return np.concatenate(\n        [\n            object_points[:, [i for i in range(3) if i != common_axis]],\n            np.zeros((object_points.shape[0], 1)),\n        ],\n        axis=1,\n    ).astype(np.float32)\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.denormalize_as_named_points","title":"<code>denormalize_as_named_points(normalised_named_points, image_width, image_height)</code>","text":"<p>Transforms a dict of normalized points <code>0 to 1</code> to image points using the supplied image dimension.</p> <p>Parameters:</p> Name Type Description Default <code>normalised_named_points</code> <code>dict[str, Point2D]</code> <p>Dict of points normalised from <code>0.0</code> to <code>1.0</code></p> required <code>image_width</code> <code>int</code> <p>Image width to expand to.</p> required <code>image_height</code> <code>int</code> <p>Image height to expand to.</p> required <p>Returns:</p> Type Description <code>dict[str, Point2D]</code> <p>dict[str, Point2D]: Retruns a dict of similar struture but with image points.</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def denormalize_as_named_points(\n    normalised_named_points: dict[str, Point2D], image_width: int, image_height: int\n) -&gt; dict[str, Point2D]:\n\"\"\"Transforms a dict of normalized points `0 to 1` to image points using the\n    supplied image dimension.\n\n    Args:\n        normalised_named_points (dict[str, Point2D]): Dict of points normalised from `0.0` to `1.0`\n        image_width (int): Image width to expand to.\n        image_height (int): Image height to expand to.\n\n    Returns:\n        dict[str, Point2D]: Retruns a dict of similar struture but with image points.\n    \"\"\"\n    return {\n        k: (v[0] * image_width, v[1] * image_height)\n        for k, v in normalised_named_points.items()\n    }\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.find_optimal_calibration_and_pose","title":"<code>find_optimal_calibration_and_pose(valid_clip_ids, calibration_correspondences, pose_correspondences, image_width, image_height, all_image_points, all_world_points)</code>","text":"<p>Givern a set of calibration and pose correspondences, find the optimal calibration and pose. This is done by building up combinations of these sets and evaluating the reprojection error. The reprojection error is the mean of the euclidean distance between the reprojected points and the actual points. The evvaluation is on all <code>all_image_points</code> and <code>all_world_points</code>.</p> <p>Parameters:</p> Name Type Description Default <code>valid_clip_ids</code> <code>set[str]</code> <p>description</p> required <code>calibration_correspondences</code> <code>list[tuple[np.array, np.array]]</code> <p>description</p> required <code>pose_correspondences</code> <code>list[tuple[np.array, np.array]]</code> <p>description</p> required <code>image_width</code> <code>int</code> <p>Image width</p> required <code>image_height</code> <code>int</code> <p>Image height</p> required <code>all_image_points</code> <code>np.array</code> <p>3D points that we want to reproject.</p> required <code>all_world_points</code> <code>np.array</code> <p>2D points that are where we expect the 3D points to be reprojected to.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>CameraInfo</code> <code>CameraInfo</code> <p>description</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def find_optimal_calibration_and_pose(\n    valid_clip_ids: set[str],\n    calibration_correspondences: list[tuple[np.array, np.array]],\n    pose_correspondences: list[tuple[np.array, np.array]],\n    image_width: int,\n    image_height: int,\n    all_image_points: np.array,\n    all_world_points: np.array,\n) -&gt; CameraInfo:\n\"\"\"\n    Givern a set of calibration and pose correspondences, find the optimal calibration and pose.\n    This is done by building up combinations of these sets and evaluating the reprojection error.\n    The reprojection error is the mean of the euclidean distance between the reprojected points and the actual points.\n    The evvaluation is on all `all_image_points` and `all_world_points`.\n\n    Args:\n        valid_clip_ids (set[str]): _description_\n        calibration_correspondences (list[tuple[np.array, np.array]]): _description_\n        pose_correspondences (list[tuple[np.array, np.array]]): _description_\n        image_width (int): Image width\n        image_height (int): Image height\n        all_image_points (np.array): 3D points that we want to reproject.\n        all_world_points (np.array): 2D points that are where we expect the 3D points to be reprojected to.\n\n    Raises:\n        RuntimeError: _description_\n\n    Returns:\n        CameraInfo: _description_\n    \"\"\"\n    CALIBRATION_MIN_PAIRS = 4\n    CALIBRATION_MAX_PAIRS = min(8, len(calibration_correspondences))\n\n    POSE_MIN_PAIRS = 4\n    POSE_MAX_PAIRS = min(8, len(pose_correspondences))\n\n    calibration_indexes = [o for o in range(len(calibration_correspondences))]\n    calibration_selected_pairs: list[tuple[int, ...]] = list(\n        chain.from_iterable(\n            (combinations(calibration_indexes, num_pairs_to_use))\n            for num_pairs_to_use in range(CALIBRATION_MIN_PAIRS, CALIBRATION_MAX_PAIRS)\n        )\n    )\n\n    pose_indexes = [o for o in range(len(pose_correspondences))]\n    pose_selected_pairs: list[tuple[int, ...]] = list(\n        chain.from_iterable(\n            (combinations(pose_indexes, num_pairs_to_use))\n            for num_pairs_to_use in range(POSE_MIN_PAIRS, POSE_MAX_PAIRS)\n        )\n    )\n\n    best_error_in_reprojected_points = 10000.0\n    best_camera_info = None\n\n    for calibration_pair, pose_pair in product(\n        calibration_selected_pairs, pose_selected_pairs\n    ):\n        calibration_correspondences_selection = [\n            calibration_correspondences[o] for o in calibration_pair\n        ]\n        pose_correspondences_selection = [pose_correspondences[o] for o in pose_pair]\n\n        camera_info = calibrate_and_evaluate(\n            valid_clip_ids=valid_clip_ids,\n            calibration_correspondences_selected=calibration_correspondences_selection,\n            pose_correspondences_selected=pose_correspondences_selection,\n            image_width=image_width,\n            image_height=image_height,\n            all_image_points=all_image_points,\n            all_world_points=all_world_points,\n        )\n        if camera_info.error_in_reprojected_points &lt; best_error_in_reprojected_points:\n            best_camera_info = camera_info\n    if best_camera_info is None:\n        raise RuntimeError(\"Failed to find optimal calibration and pose\")\n    return best_camera_info\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.get_planar_point_correspondences","title":"<code>get_planar_point_correspondences(world_points, image_points, available_labels=None, minimal_set_count=4)</code>","text":"<p>Given a set of named points in the world and image, return a list of point correspondences where all points are coplanar. If a specified set <code>available_labels</code> is given, only return point correspondences where all points are in that set.</p> <p>Parameters:</p> Name Type Description Default <code>world_points</code> <code>dict[str, tuple[float, float]]</code> <p>Dict of named points in the world coordinate frame.</p> required <code>image_points</code> <code>dict[str, tuple[float, float]]</code> <p>Dict of named points in the image coordinate frame.</p> required <code>available_labels</code> <code>Optional[set[str]]</code> <p>Set of labels to use if None all labels are used. Defaults to None.</p> <code>None</code> <code>minimal_set_count</code> <code>int</code> <p>Sets of corresponding points . Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>list[tuple[np.ndarray, np.ndarray]]</code> <p>list[tuple[np.ndarray, np.ndarray]]: Returns a list of point correspondences where all points are coplanar.</p> <code>list[tuple[np.ndarray, np.ndarray]]</code> <p>list[tuple[Nx3, Nx2]]</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def get_planar_point_correspondences(\n    world_points: dict[str, tuple[float, float]],\n    image_points: dict[str, tuple[float, float]],\n    available_labels: Optional[set[str]] = None,\n    minimal_set_count: int = 4,\n) -&gt; list[tuple[np.ndarray, np.ndarray]]:\n\"\"\"Given a set of named points in the world and image, return a list of point correspondences\n    where all points are coplanar.\n    If a specified set `available_labels` is given, only return point correspondences where all\n    points are in that set.\n    Args:\n        world_points (dict[str, tuple[float, float]]): Dict of named points in the world coordinate frame.\n        image_points (dict[str, tuple[float, float]]): Dict of named points in the image coordinate frame.\n        available_labels (Optional[set[str]], optional): Set of labels to use if None all labels are used. Defaults to None.\n        minimal_set_count (int, optional): Sets of corresponding points . Defaults to 4.\n\n    Returns:\n        list[tuple[np.ndarray, np.ndarray]]: Returns a list of point correspondences where all points are coplanar.\n        list[tuple[Nx3, Nx2]]\n    \"\"\"\n    available_labels = available_labels or set(image_points.keys())\n    available_planes_for_calibration = get_planar_points_padel_court(\n        available_labels=available_labels,\n        minimal_set_count=minimal_set_count,\n    )\n    from courtvision.data import dict_to_points\n\n    planar_point_correspondences = []\n    for plane in available_planes_for_calibration:\n        world_points_on_plane_dict = {k: world_points[k] for k in plane}\n        image_points_on_plane_dict = {k: image_points[k] for k in plane}\n        world_points_on_plane, _ = dict_to_points(world_points_on_plane_dict)\n        image_points_on_plane, _ = dict_to_points(image_points_on_plane_dict)\n        planar_point_correspondences.append(\n            (world_points_on_plane, image_points_on_plane)\n        )\n    return planar_point_correspondences\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.project_points_to_base_plane","title":"<code>project_points_to_base_plane(points, H)</code>","text":"<p>Given homogeneous points or 2D points and a homography, project the points to the base plane</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>torch.tensor</code> <p>Homogeneous points or 2D points</p> required <code>H</code> <code>torch.tensor</code> <p>Homography 3x3 matrix</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>points</code> is not of length 2 or 3</p> <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: Projected points in either homogeneous or 2D corrdinates. Same as <code>points</code></p> Source code in <code>courtvision/geometry.py</code> <pre><code>def project_points_to_base_plane(points: torch.tensor, H: torch.tensor) -&gt; torch.tensor:\n\"\"\"Given homogeneous points or 2D points and a homography, project the points to the base plane\n\n    Args:\n        points (torch.tensor): Homogeneous points or 2D points\n        H (torch.tensor): Homography 3x3 matrix\n\n    Raises:\n        ValueError: If `points` is not of length 2 or 3\n\n    Returns:\n        torch.tensor: Projected points in either homogeneous or 2D corrdinates. Same as `points`\n    \"\"\"\n    if len(points.shape) == 2:\n        return convert_points_from_homogeneous(\n            convert_points_to_homogeneous(points) @ H.T\n        )\n    elif len(points.shape) == 3:\n        return points @ H.T\n    else:\n        raise ValueError(f\"{points.shape=} must be of length 2 or 3.\")\n</code></pre>"},{"location":"api/geometry/#courtvision.geometry.solve_for_camera_matrix","title":"<code>solve_for_camera_matrix(world_points, image_points, image_size, repo_erro_threshold=0.1)</code>","text":"<p>From a set of world points and image points, solve for the camera matrix and distortion coefficients. Note: All world points must have the same z value. i.e lie on the same plane.</p> <p>Parameters:</p> Name Type Description Default <code>world_points</code> <code>torch.Tensor</code> <p>Tensor of world points.</p> required <code>image_points</code> <code>torch.Tensor</code> <p>Tensor of image points.</p> required <code>image_size</code> <code>tuple[int, int]</code> <p>Image dimensions as (Width, Height).</p> required <code>repo_error</code> <code>float</code> <p>Reprojection error measured in pixels. Defaults to 1e-1.</p> required <p>Returns (Tuple[torch.Tensor, torch.Tensor, float]): camera_matrix (3x3), dist_coeffs (1x5), repo_erro</p> Source code in <code>courtvision/geometry.py</code> <pre><code>def solve_for_camera_matrix(\n    world_points: torch.Tensor,\n    image_points: torch.Tensor,\n    image_size: tuple[int, int],\n    repo_erro_threshold: float = 1e-1,\n) -&gt; tuple[torch.Tensor, torch.Tensor, float]:\n\"\"\"From a set of world points and image points, solve for the camera matrix and distortion coefficients.\n    Note: All world points must have the same z value. i.e lie on the same plane.\n\n    Args:\n        world_points (torch.Tensor): Tensor of world points.\n        image_points (torch.Tensor): Tensor of image points.\n        image_size (tuple[int, int]): Image dimensions as (Width, Height).\n        repo_error (float, optional): Reprojection error measured in pixels. Defaults to 1e-1.\n\n    Returns (Tuple[torch.Tensor, torch.Tensor, float]): camera_matrix (3x3), dist_coeffs (1x5), repo_erro\n\n    \"\"\"\n    if len(world_points.shape) == 3:\n        _world_points = [world_points.squeeze(0).numpy().astype(np.float32)]\n    elif len(world_points.shape) == 2:\n        _world_points = [world_points.numpy().astype(np.float32)]\n    else:\n        raise RuntimeError(f\"{world_points.shape=} must be of length 2 or 3.\")\n    if len(image_points.shape) == 3:\n        _image_points = [image_points.squeeze(0).numpy().astype(np.float32)]\n    elif len(image_points.shape) == 2:\n        _image_points = [image_points.numpy().astype(np.float32)]\n    else:\n        raise RuntimeError(f\"{image_points.shape=} must be of length 2 or 3.\")\n\n    # if not all(o[-1] == _world_points[0][0][-1] for o in _world_points[0]):\n    # raise RuntimeError(f\"{_world_points=} must have same z value\")\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 1000, 0.001)\n\n    repo_erro, camera_matrix, dist_coeffs, *_ = cv2.calibrateCamera(\n        objectPoints=_world_points,\n        imagePoints=_image_points,\n        imageSize=image_size,\n        cameraMatrix=None,\n        distCoeffs=None,\n        criteria=criteria,\n    )\n    if repo_erro &gt; repo_erro_threshold:\n        raise RuntimeError(f\"{repo_erro=} must be less than 1e-6\")\n    print(f\"{repo_erro=}\")\n    return camera_matrix, dist_coeffs, repo_erro\n</code></pre>"},{"location":"api/models/","title":"courtvision.models","text":""},{"location":"api/models/#courtvision.models.BallDetector","title":"<code>BallDetector</code>","text":"Source code in <code>courtvision/models.py</code> <pre><code>class BallDetector:\n    PIPELINE_NAME = \"ball_detection\"\n\n    def __init__(self, model_file_or_dir: Path, cache_dir: Path):\n        if model_file_or_dir.is_dir():\n            self.model_path = get_latest_file(model_file_or_dir)\n        else:\n            self.model_path = model_file_or_dir\n\n        self.model = get_ball_detection_model(model_path=self.model_path)\n        self.cache_dir = cache_dir\n        self.model.eval()\n\n    def predict(\n        self, image: torch.Tensor, frame_idx: int, clip_uid: str\n    ) -&gt; dict[str, torch.Tensor]:\n\"\"\"Predicts ball detections for a given frame.\n        !!! note\n            This method caches the detections on disk.\n        Args:\n            image (torch.Tensor): Image tensor of shape (1,3,H,W)\n            frame_idx (int): frame index\n            clip_uid (str): clip uid that identifies the clip uniquely.\n\n        Returns:\n            dict[str, torch.Tensor]: A dict tensor ball detections.\n        \"\"\"\n        cache_path = (\n            self.cache_dir\n            / self.PIPELINE_NAME\n            / clip_uid\n            / f\"detections_at_{frame_idx}.pt\"\n        )\n        if not cache_path.is_dir():\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n        if cache_path.is_file():\n            return torch.load(cache_path)\n        else:\n            with torch.no_grad():\n                detections = self.model(image)\n            torch.save(detections, cache_path)\n            return detections\n</code></pre>"},{"location":"api/models/#courtvision.models.BallDetector.predict","title":"<code>predict(image, frame_idx, clip_uid)</code>","text":"<p>Predicts ball detections for a given frame.</p> <p>Note</p> <p>This method caches the detections on disk.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>torch.Tensor</code> <p>Image tensor of shape (1,3,H,W)</p> required <code>frame_idx</code> <code>int</code> <p>frame index</p> required <code>clip_uid</code> <code>str</code> <p>clip uid that identifies the clip uniquely.</p> required <p>Returns:</p> Type Description <code>dict[str, torch.Tensor]</code> <p>dict[str, torch.Tensor]: A dict tensor ball detections.</p> Source code in <code>courtvision/models.py</code> <pre><code>def predict(\n    self, image: torch.Tensor, frame_idx: int, clip_uid: str\n) -&gt; dict[str, torch.Tensor]:\n\"\"\"Predicts ball detections for a given frame.\n    !!! note\n        This method caches the detections on disk.\n    Args:\n        image (torch.Tensor): Image tensor of shape (1,3,H,W)\n        frame_idx (int): frame index\n        clip_uid (str): clip uid that identifies the clip uniquely.\n\n    Returns:\n        dict[str, torch.Tensor]: A dict tensor ball detections.\n    \"\"\"\n    cache_path = (\n        self.cache_dir\n        / self.PIPELINE_NAME\n        / clip_uid\n        / f\"detections_at_{frame_idx}.pt\"\n    )\n    if not cache_path.is_dir():\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n    if cache_path.is_file():\n        return torch.load(cache_path)\n    else:\n        with torch.no_grad():\n            detections = self.model(image)\n        torch.save(detections, cache_path)\n        return detections\n</code></pre>"},{"location":"api/models/#courtvision.models.PlayerDetector","title":"<code>PlayerDetector</code>","text":"Source code in <code>courtvision/models.py</code> <pre><code>class PlayerDetector:\n    PIPELINE_NAME = \"player_detection\"\n\n    def __init__(self, model_dir: Path, cache_dir: Path):\n        self.model_path = get_latest_file(model_dir)\n        self.cache_dir = cache_dir\n        self.model = get_yolov8_player_detection_model(model_path=self.model_path)\n        # self.model.eval()\n\n    def predict(\n        self, image: torch.Tensor, frame_idx: int, clip_uid: str\n    ) -&gt; dict[str, torch.Tensor]:\n\"\"\"Predicts player detections for a given frame.\n        !!! note\n            This method caches the detections on disk.\n        Args:\n            image (torch.Tensor): Image tensor of shape (1,3,H,W)\n            frame_idx (int): frame index\n            clip_uid (str): clip uid that identifies the clip uniquely.\n\n        Returns:\n            dict[str, torch.Tensor]: Dict of player detections.\n        \"\"\"\n        cache_path = (\n            self.cache_dir\n            / self.PIPELINE_NAME\n            / clip_uid\n            / f\"detections_at_{frame_idx}.pt\"\n        )\n        if not cache_path.is_dir():\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n        if cache_path.is_file():\n            return torch.load(cache_path)\n        else:\n            with torch.no_grad():\n                detections = self.model.track(\n                    source=image.squeeze(0).permute(1, 2, 0).numpy(),\n                    persist=True,\n                )\n            torch.save(detections, cache_path)\n            return detections\n</code></pre>"},{"location":"api/models/#courtvision.models.PlayerDetector.predict","title":"<code>predict(image, frame_idx, clip_uid)</code>","text":"<p>Predicts player detections for a given frame.</p> <p>Note</p> <p>This method caches the detections on disk.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>torch.Tensor</code> <p>Image tensor of shape (1,3,H,W)</p> required <code>frame_idx</code> <code>int</code> <p>frame index</p> required <code>clip_uid</code> <code>str</code> <p>clip uid that identifies the clip uniquely.</p> required <p>Returns:</p> Type Description <code>dict[str, torch.Tensor]</code> <p>dict[str, torch.Tensor]: Dict of player detections.</p> Source code in <code>courtvision/models.py</code> <pre><code>def predict(\n    self, image: torch.Tensor, frame_idx: int, clip_uid: str\n) -&gt; dict[str, torch.Tensor]:\n\"\"\"Predicts player detections for a given frame.\n    !!! note\n        This method caches the detections on disk.\n    Args:\n        image (torch.Tensor): Image tensor of shape (1,3,H,W)\n        frame_idx (int): frame index\n        clip_uid (str): clip uid that identifies the clip uniquely.\n\n    Returns:\n        dict[str, torch.Tensor]: Dict of player detections.\n    \"\"\"\n    cache_path = (\n        self.cache_dir\n        / self.PIPELINE_NAME\n        / clip_uid\n        / f\"detections_at_{frame_idx}.pt\"\n    )\n    if not cache_path.is_dir():\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n    if cache_path.is_file():\n        return torch.load(cache_path)\n    else:\n        with torch.no_grad():\n            detections = self.model.track(\n                source=image.squeeze(0).permute(1, 2, 0).numpy(),\n                persist=True,\n            )\n        torch.save(detections, cache_path)\n        return detections\n</code></pre>"},{"location":"api/models/#courtvision.models.get_ball_detection_model","title":"<code>get_ball_detection_model(model_path)</code>","text":"<p>Grabs a trained ball detection model from a path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>Path to the model weights. A .ckpt file.</p> required <p>Returns:</p> Name Type Description <code>BallDetectorModel</code> <code>BallDetectorModel</code> <p>A trained BallDetectorModel from a checkpoint.</p> Source code in <code>courtvision/models.py</code> <pre><code>def get_ball_detection_model(model_path: Path) -&gt; \"BallDetectorModel\":\n\"\"\"Grabs a trained ball detection model from a path.\n\n    Args:\n        model_path (Path): Path to the model weights. A .ckpt file.\n\n    Returns:\n        BallDetectorModel: A trained BallDetectorModel from a checkpoint.\n    \"\"\"\n    from courtvision.trainer import BallDetectorModel  # TODO: move to models.py\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    return BallDetectorModel.load_from_checkpoint(model_path, map_location=device)\n</code></pre>"},{"location":"api/models/#courtvision.models.get_fasterrcnn_ball_detection_model","title":"<code>get_fasterrcnn_ball_detection_model(model_path=None)</code>","text":"<p>Fetches a FasterRCNN model for ball detection. If model_path is None, the model is pretrained on COCO. If model_path is a Path, the model is loaded from the path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>None | Path</code> <p>Path do model weights that will be loaded. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FasterRCNN</code> <code>FasterRCNN</code> <p>A ball detection model using FasterRCNN</p> Source code in <code>courtvision/models.py</code> <pre><code>def get_fasterrcnn_ball_detection_model(model_path: None | Path = None) -&gt; FasterRCNN:\n\"\"\"Fetches a FasterRCNN model for ball detection.\n    If model_path is None, the model is pretrained on COCO.\n    If model_path is a Path, the model is loaded from the path.\n\n    Args:\n        model_path (None | Path, optional): Path do model weights that will be loaded. Defaults to None.\n\n    Returns:\n        FasterRCNN: A ball detection model using FasterRCNN\n    \"\"\"\n\n    pretrained = model_path is None\n\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n        weights=None, weights_backbone=None, pretrained=pretrained\n    )\n    num_classes = 2  # 1 class (ball) + background\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    if model_path is not None:\n        model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n    return model\n</code></pre>"},{"location":"api/models/#courtvision.models.get_yolo_player_detection_model","title":"<code>get_yolo_player_detection_model(model_path=None)</code>","text":"<p>Fetches a pretrained YOLO model for player detection.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>None | Path</code> <p>Unused!. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Yolo model</p> Source code in <code>courtvision/models.py</code> <pre><code>def get_yolo_player_detection_model(model_path: None | Path = None) -&gt; Any:\n\"\"\"Fetches a pretrained YOLO model for player detection.\n\n    Args:\n        model_path (None | Path, optional): Unused!. Defaults to None.\n\n    Returns:\n        Any: Yolo model\n    \"\"\"\n    model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n    return model\n</code></pre>"},{"location":"api/pipeline/","title":"Example usage of CourtVision","text":"<p>Set config in a <code>.env</code> file. <pre><code>python pipeline.py\n</code></pre> </p>"},{"location":"api/swiss/","title":"courtvision.swiss","text":""},{"location":"api/swiss/#courtvision.swiss.get_latest_file","title":"<code>get_latest_file(dir, file_suffix='.pt')</code>","text":"<p>Fetch the model_path of the latest model in <code>model_dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>Path</code> <p>path to directory of models.</p> required <code>model_suffix</code> <code>str</code> <p>extention of model format. Defaults to \".pt\".</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>path to most recent model.</p> Source code in <code>courtvision/swiss.py</code> <pre><code>def get_latest_file(dir: Path, file_suffix: str = \".pt\") -&gt; Path:\n\"\"\"Fetch the model_path of the latest model in `model_dir`.\n\n    Args:\n        model_dir (Path): path to directory of models.\n        model_suffix (str, optional): extention of model format. Defaults to \".pt\".\n\n    Returns:\n        Path: path to most recent model.\n    \"\"\"\n\n    # Get the most recent file based on modification time\n    most_recent_file = None\n    most_recent_time = datetime.datetime.min\n    for file in dir.glob(f\"*{file_suffix}\"):\n        modification_time = datetime.datetime.fromtimestamp(os.path.getmtime(file))\n        if modification_time &gt; most_recent_time:\n            most_recent_time = modification_time\n            most_recent_file = file\n    return most_recent_file\n</code></pre>"},{"location":"api/swiss/#courtvision.swiss.mark_as_deprecated","title":"<code>mark_as_deprecated(to_be_removed_in_version, details, moved_to=None)</code>","text":"<p>Marks a function as deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>to_be_removed_in_version</code> <code>tuple[int, int, int]</code> <p>after which version the function will be removed.</p> required <code>details</code> <code>str</code> <p>Message for the caller.</p> required <code>moved_to</code> <code>Optional[str]</code> <p>If a function exists that callers should migrate to. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <p>function wrapper</p> Source code in <code>courtvision/swiss.py</code> <pre><code>def mark_as_deprecated(\n    to_be_removed_in_version: tuple[int, int, int],\n    details: str,\n    moved_to: Optional[str] = None,\n):\n\"\"\"Marks a function as deprecated.\n    Args:\n        to_be_removed_in_version (tuple[int, int, int]): after which version the function will be removed.\n        details (str): Message for the caller.\n        moved_to (Optional[str], optional): If a function exists that callers should migrate to. Defaults to None.\n\n    Returns:\n        Callable: function wrapper\n    \"\"\"\n\n    def inner(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated and will be removed in Version {to_be_removed_in_version}. Details {details}\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return inner\n</code></pre>"},{"location":"api/swiss/#courtvision.swiss.save_camera_params","title":"<code>save_camera_params(*, file_name, homography, intrinsic_matrix=None, distortion_coeffs=None)</code>","text":"<p>Save camera parameters to a file.</p> Source code in <code>courtvision/swiss.py</code> <pre><code>def save_camera_params(\n    *, file_name, homography, intrinsic_matrix=None, distortion_coeffs=None\n):\n\"\"\"Save camera parameters to a file.\"\"\"\n    file_path = Path(file_name)\n    if intrinsic_matrix is not None:\n        np.save(file_path.parent / \"intrinsic_matrix\", intrinsic_matrix)\n    if distortion_coeffs is not None:\n        np.save(file_path.parent / \"distortion_coeffs\", distortion_coeffs)\n    np.save(file_path.parent / \"homography\", homography)\n</code></pre>"},{"location":"api/trackers/","title":"courtvision.trackers","text":""},{"location":"api/trackers/#courtvision.trackers.ParticleFilter","title":"<code>ParticleFilter</code>","text":"<p>Particle filter tracker.</p> Source code in <code>courtvision/trackers.py</code> <pre><code>class ParticleFilter:\n\"\"\"\n    Particle filter tracker.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        num_particles: int,\n        court_size: torch.Tensor,\n        world_to_cam: torch.Tensor | None = None,\n        cam_to_image: torch.Tensor | None = None,\n    ) -&gt; None:\n        self.reset(\n            num_particles=num_particles,\n            court_size=court_size,\n            world_to_cam=world_to_cam,\n            cam_to_image=cam_to_image,\n        )\n\n    def reset(\n        self,\n        *,\n        num_particles: int,\n        court_size: torch.tensor,\n        world_to_cam: torch.Tensor | None = None,\n        cam_to_image: torch.Tensor | None = None,\n    ) -&gt; None:\n        self.num_particles = num_particles\n        # Court size is a tensor of the form [width, length, height]\n        self.court_size = court_size\n\n        # state: [x, y, z, vx, vy, zv, ax, ay, az, ax, ay, az, weight]\n        self.states = torch.randn(\n            (num_particles, 3 * 3 + 1),\n            names=[\"num_particles\", \"state\"],\n        )\n\n        self.states[:, StateIdx.x] = (\n            self.states[:, StateIdx.x] * court_size[StateIdx.x]\n            + court_size[StateIdx.x] / 2\n        )\n        self.states[:, StateIdx.y] = (\n            self.states[:, StateIdx.y] * court_size[StateIdx.y]\n            + court_size[StateIdx.y] / 2\n        )\n        self.states[:, StateIdx.z] = (\n            self.states[:, StateIdx.z] * court_size[StateIdx.z]\n            + court_size[StateIdx.z] / 2\n        )\n        self.states[:, StateIdx.weight] = torch.abs(\n            self.normalized_weights(self.states)\n        )\n        # Set the initial velocity to be zero\n        self.states[:, StateIdx.vx : StateIdx.vz + 1] = 0.0\n        # Set the initial acceleration to be zero but -9.8 m/s^2 in the z direction\n        self.states[:, StateIdx.ax : StateIdx.az + 1] = 0.0\n        self.states[:, StateIdx.az] = -0.98  # Note: units are in 1e-1 m/s^2\n\n        self.cam_to_image = torch.randn((3, 3))\n        if cam_to_image is not None:\n            self.cam_to_image = cam_to_image.to(dtype=torch.float32)\n        self.world_to_cam = torch.randn((4, 4))\n        if world_to_cam is not None:\n            self.world_to_cam = world_to_cam.to(dtype=torch.float32)\n\n    def set_states_to(self, point: torch.tensor):\n\"\"\"Set the state of the tracker to a single point.\n\n        Args:\n            point (torch.tensor): point in the world space. Shape: [3]\n        \"\"\"\n        self.states[:, StateIdx.x : StateIdx.z + 1] = point.repeat(\n            (self.num_particles, 1)\n        )\n\n    @property\n    def xyz(self) -&gt; torch.tensor:\n\"\"\"Grab the xyz coordinates of the tracker.\n\n        Returns:\n            torch.tensor: [X,Y,Z] coordinates of the tracker. Shape: [N, 3]\n        \"\"\"\n        return self.states[:, StateIdx.x : StateIdx.z + 1].clone()\n\n    @property\n    def mean_image_plane_prediction(self) -&gt; torch.tensor:\n\"\"\"Computes the weighted mean of the trackers state and\n        projects it to the image plane.\n\n        Returns:\n            torch.tensor: [x,y] coordinates of the tracker mean estimate in the image plane. Shape: [1, 2]\n        \"\"\"\n        return self.state_to_observation(\n            self.xyz_mean,\n            world_to_cam=self.world_to_cam,\n            cam_to_image=self.cam_to_image,\n        )\n\n    @property\n    def xyz_mean(self) -&gt; torch.tensor:\n\"\"\"Grab the weighted mean of the xyz coordinates of the tracker.\n\n        Returns:\n            torch.tensor: Weighted mean of the [X,Y,Z] coordinates of the tracker. Shape: [1, 3]\n        \"\"\"\n        xyz_mean = (\n            self.xyz.rename(None).T @ self.states[:, StateIdx.weight].rename(None)\n        ) / self.states[:, StateIdx.weight].rename(None).sum()\n        return xyz_mean.unsqueeze(0)\n\n    @staticmethod\n    def normalized_weights(states: torch.tensor) -&gt; torch.tensor:\n\n        return (\n            states.select(\"state\", StateIdx.weight).rename(None)\n            / states.select(\"state\", StateIdx.weight).rename(None).sum()\n        )\n\n    @staticmethod\n    def state_to_observation(\n        state: torch.Tensor,\n        *,\n        world_to_cam: torch.Tensor,\n        cam_to_image: torch.Tensor,\n    ) -&gt; torch.Tensor:\n\"\"\"Map the state to the observation space.\n        This is from the 3D world space to the 2D image plane.\n\n        Args:\n            state (torch.Tensor): Tracker state. Shape: [N, state_dim]\n            world_to_cam (torch.Tensor): _description_\n            cam_to_image (torch.Tensor): _description_\n\n        Returns:\n            torch.Tensor: _description_\n        \"\"\"\n        x_y_z_1_positions = convert_points_to_homogeneous(\n            state[:, : StateIdx.z + 1].rename(None)\n        )\n        x_y_z_1_positions_cam = convert_points_from_homogeneous(\n            x_y_z_1_positions @ world_to_cam.T\n        )\n        return convert_points_from_homogeneous(x_y_z_1_positions_cam @ cam_to_image.T)\n\n    def likelihood_in_state_space(\n        self, obs_state: torch.tensor, pred_state: torch.tensor\n    ):\n        # unproject_points\n        ...\n\n    # # Define the likelihood function\n    def likelihood(\n        self, obs_state: torch.tensor, pred_state: torch.tensor\n    ) -&gt; torch.tensor:\n\"\"\"Compute the likelihood of the observation given the predicted state.\n\n        Args:\n            obs_state (torch.tensor): Observation in the image plane. Shape: [2]\n            pred_state (torch.tensor): Predicted state. Shape: [N, state_dim]\n\n        Returns:\n            orch.tensor: likelihood of the observation given the predicted state. Shape: [N]\n        \"\"\"\n        sigma = 20.1  # Standard deviation of the observation noise\n        from torch.nn.functional import mse_loss\n\n        pred_obs = self.state_to_observation(\n            pred_state, world_to_cam=self.world_to_cam, cam_to_image=self.cam_to_image\n        )\n        mse = mse_loss(pred_obs, obs_state.expand_as(pred_obs), reduction=\"none\").sum(\n            dim=1\n        )\n        p = torch.max(torch.exp(-0.5 * mse / sigma) ** 2, torch.tensor(1e-3))\n        return p\n\n    def predict(self, dt: float = 1.0 / 30.0):\n\"\"\"\n        Predict the next state using the current state and the time step.\n        `p(x_t | x_{t-1}) ~ N(x_t; x_{t-1} + v_{t-1} * dt, sigma^2)`\n\n\n        Args:\n            dt (float, optional): Time step in [s]. Defaults to 1.0/30.0.\n        \"\"\"\n\n        # state: [x, y, z, vx, vy, zv, ax, ay, az, ax, ay, az, weight]\n        # Random walk in the x, y, and z directions\n        # self.states[:, StateIdx.x : StateIdx.z + 1] = (\n        #     self.states[:, StateIdx.x : StateIdx.z + 1]\n        #     + torch.randn((self.num_particles, 3)) * 5.0\n        # )\n\n        # Update the state using the velocity\n        self.states[:, StateIdx.x : StateIdx.z + 1] += (\n            self.states[:, StateIdx.vx : StateIdx.vz + 1] * dt\n        ) + 0.5 * (self.states[:, StateIdx.ax : StateIdx.az + 1] * dt**2)\n\n        # Ensure state is on the court using clamp.\n        self.states[:, StateIdx.x] = torch.clamp(\n            self.states[:, StateIdx.x], 0.0, self.court_size[StateIdx.x]\n        )\n        self.states[:, StateIdx.y] = torch.clamp(\n            self.states[:, StateIdx.y], 0.0, self.court_size[StateIdx.y]\n        )\n        self.states[:, StateIdx.z] = torch.clamp(\n            self.states[:, StateIdx.z], 0.0, self.court_size[StateIdx.z] * 1.5\n        )\n\n        # Update the velocity using the acceleration + jitter\n        self.states[:, StateIdx.vx : StateIdx.vz + 1] += (\n            self.states[:, StateIdx.ax : StateIdx.az + 1] * dt\n            + torch.randn((self.num_particles, 3)) * 1.5\n        )\n\n        self.states[:, StateIdx.ax : StateIdx.az + 1] = (\n            torch.randn((self.num_particles, 3)) * 0.9\n        )\n        self.states[:, StateIdx.az] = -0.98  # Note: units are in 1e-1 m/s^2\n\n    def update(self, obs_state: torch.tensor, score: torch.tensor = torch.tensor(1.0)):\n\"\"\"Update the state using the observation and it's associated score.\n\n        Args:\n            obs_state (torch.tensor): measurement in the image plane. Shape: [2]\n            score (torch.tensor, optional): If the detector assigns a score this\n                                            can be used in the update step.\n                                            Defaults to torch.tensor(1.0).\n        \"\"\"\n        likelihoods = self.likelihood(obs_state, self.states) * score\n        self.states[:, StateIdx.weight] = self.update_weights(\n            self.states[:, StateIdx.weight], likelihoods\n        )\n        self.states = self.resample(self.states, self.states[:, StateIdx.weight])\n\n    @staticmethod\n    def update_weights(\n        weights: torch.tensor, likelihoods: torch.tensor\n    ) -&gt; torch.tensor:\n\"\"\"Given the current weights and the likelihoods, update the weights.\n\n        Args:\n            weights (torch.tensor): Nx1 tensor of weights\n            likelihoods (torch.tensor): Nx1 tensor of likelihoods\n\n        Returns:\n            torch.tensor: Nx1 tensor of updated weights\n        \"\"\"\n        return (\n            weights.rename(None)\n            * likelihoods\n            / (weights.rename(None) * likelihoods).sum()\n        )\n\n    @staticmethod\n    def resample(states: torch.tensor, weights: torch.tensor) -&gt; torch.tensor:\n\"\"\"Given a set of states and associated weights, resample the states.\n\n        Args:\n            states (torch.tensor): Tracker state. Shape: [N, state_dim]\n            weights (torch.tensor): weights associated with each state. Shape: [N x 1]\n\n        Returns:\n            torch.tensor: returns the resampled states. Shape: [N, state_dim]\n        \"\"\"\n        if weights.names is not None:\n            weights = weights.rename(None)\n        if states.names is not None:\n            states = states.rename(None)\n\n        return states[\n            torch.multinomial(weights, len(weights), replacement=True)\n        ].rename(\"num_particles\", \"state\")\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.mean_image_plane_prediction","title":"<code>mean_image_plane_prediction: torch.tensor</code>  <code>property</code>","text":"<p>Computes the weighted mean of the trackers state and projects it to the image plane.</p> <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: [x,y] coordinates of the tracker mean estimate in the image plane. Shape: [1, 2]</p>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.xyz","title":"<code>xyz: torch.tensor</code>  <code>property</code>","text":"<p>Grab the xyz coordinates of the tracker.</p> <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: [X,Y,Z] coordinates of the tracker. Shape: [N, 3]</p>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.xyz_mean","title":"<code>xyz_mean: torch.tensor</code>  <code>property</code>","text":"<p>Grab the weighted mean of the xyz coordinates of the tracker.</p> <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: Weighted mean of the [X,Y,Z] coordinates of the tracker. Shape: [1, 3]</p>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.likelihood","title":"<code>likelihood(obs_state, pred_state)</code>","text":"<p>Compute the likelihood of the observation given the predicted state.</p> <p>Parameters:</p> Name Type Description Default <code>obs_state</code> <code>torch.tensor</code> <p>Observation in the image plane. Shape: [2]</p> required <code>pred_state</code> <code>torch.tensor</code> <p>Predicted state. Shape: [N, state_dim]</p> required <p>Returns:</p> Type Description <code>torch.tensor</code> <p>orch.tensor: likelihood of the observation given the predicted state. Shape: [N]</p> Source code in <code>courtvision/trackers.py</code> <pre><code>def likelihood(\n    self, obs_state: torch.tensor, pred_state: torch.tensor\n) -&gt; torch.tensor:\n\"\"\"Compute the likelihood of the observation given the predicted state.\n\n    Args:\n        obs_state (torch.tensor): Observation in the image plane. Shape: [2]\n        pred_state (torch.tensor): Predicted state. Shape: [N, state_dim]\n\n    Returns:\n        orch.tensor: likelihood of the observation given the predicted state. Shape: [N]\n    \"\"\"\n    sigma = 20.1  # Standard deviation of the observation noise\n    from torch.nn.functional import mse_loss\n\n    pred_obs = self.state_to_observation(\n        pred_state, world_to_cam=self.world_to_cam, cam_to_image=self.cam_to_image\n    )\n    mse = mse_loss(pred_obs, obs_state.expand_as(pred_obs), reduction=\"none\").sum(\n        dim=1\n    )\n    p = torch.max(torch.exp(-0.5 * mse / sigma) ** 2, torch.tensor(1e-3))\n    return p\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.predict","title":"<code>predict(dt=1.0 / 30.0)</code>","text":"<p>Predict the next state using the current state and the time step. <code>p(x_t | x_{t-1}) ~ N(x_t; x_{t-1} + v_{t-1} * dt, sigma^2)</code></p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>float</code> <p>Time step in [s]. Defaults to 1.0/30.0.</p> <code>1.0 / 30.0</code> Source code in <code>courtvision/trackers.py</code> <pre><code>def predict(self, dt: float = 1.0 / 30.0):\n\"\"\"\n    Predict the next state using the current state and the time step.\n    `p(x_t | x_{t-1}) ~ N(x_t; x_{t-1} + v_{t-1} * dt, sigma^2)`\n\n\n    Args:\n        dt (float, optional): Time step in [s]. Defaults to 1.0/30.0.\n    \"\"\"\n\n    # state: [x, y, z, vx, vy, zv, ax, ay, az, ax, ay, az, weight]\n    # Random walk in the x, y, and z directions\n    # self.states[:, StateIdx.x : StateIdx.z + 1] = (\n    #     self.states[:, StateIdx.x : StateIdx.z + 1]\n    #     + torch.randn((self.num_particles, 3)) * 5.0\n    # )\n\n    # Update the state using the velocity\n    self.states[:, StateIdx.x : StateIdx.z + 1] += (\n        self.states[:, StateIdx.vx : StateIdx.vz + 1] * dt\n    ) + 0.5 * (self.states[:, StateIdx.ax : StateIdx.az + 1] * dt**2)\n\n    # Ensure state is on the court using clamp.\n    self.states[:, StateIdx.x] = torch.clamp(\n        self.states[:, StateIdx.x], 0.0, self.court_size[StateIdx.x]\n    )\n    self.states[:, StateIdx.y] = torch.clamp(\n        self.states[:, StateIdx.y], 0.0, self.court_size[StateIdx.y]\n    )\n    self.states[:, StateIdx.z] = torch.clamp(\n        self.states[:, StateIdx.z], 0.0, self.court_size[StateIdx.z] * 1.5\n    )\n\n    # Update the velocity using the acceleration + jitter\n    self.states[:, StateIdx.vx : StateIdx.vz + 1] += (\n        self.states[:, StateIdx.ax : StateIdx.az + 1] * dt\n        + torch.randn((self.num_particles, 3)) * 1.5\n    )\n\n    self.states[:, StateIdx.ax : StateIdx.az + 1] = (\n        torch.randn((self.num_particles, 3)) * 0.9\n    )\n    self.states[:, StateIdx.az] = -0.98  # Note: units are in 1e-1 m/s^2\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.resample","title":"<code>resample(states, weights)</code>  <code>staticmethod</code>","text":"<p>Given a set of states and associated weights, resample the states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>torch.tensor</code> <p>Tracker state. Shape: [N, state_dim]</p> required <code>weights</code> <code>torch.tensor</code> <p>weights associated with each state. Shape: [N x 1]</p> required <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: returns the resampled states. Shape: [N, state_dim]</p> Source code in <code>courtvision/trackers.py</code> <pre><code>@staticmethod\ndef resample(states: torch.tensor, weights: torch.tensor) -&gt; torch.tensor:\n\"\"\"Given a set of states and associated weights, resample the states.\n\n    Args:\n        states (torch.tensor): Tracker state. Shape: [N, state_dim]\n        weights (torch.tensor): weights associated with each state. Shape: [N x 1]\n\n    Returns:\n        torch.tensor: returns the resampled states. Shape: [N, state_dim]\n    \"\"\"\n    if weights.names is not None:\n        weights = weights.rename(None)\n    if states.names is not None:\n        states = states.rename(None)\n\n    return states[\n        torch.multinomial(weights, len(weights), replacement=True)\n    ].rename(\"num_particles\", \"state\")\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.set_states_to","title":"<code>set_states_to(point)</code>","text":"<p>Set the state of the tracker to a single point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>torch.tensor</code> <p>point in the world space. Shape: [3]</p> required Source code in <code>courtvision/trackers.py</code> <pre><code>def set_states_to(self, point: torch.tensor):\n\"\"\"Set the state of the tracker to a single point.\n\n    Args:\n        point (torch.tensor): point in the world space. Shape: [3]\n    \"\"\"\n    self.states[:, StateIdx.x : StateIdx.z + 1] = point.repeat(\n        (self.num_particles, 1)\n    )\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.state_to_observation","title":"<code>state_to_observation(state, *, world_to_cam, cam_to_image)</code>  <code>staticmethod</code>","text":"<p>Map the state to the observation space. This is from the 3D world space to the 2D image plane.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>torch.Tensor</code> <p>Tracker state. Shape: [N, state_dim]</p> required <code>world_to_cam</code> <code>torch.Tensor</code> <p>description</p> required <code>cam_to_image</code> <code>torch.Tensor</code> <p>description</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: description</p> Source code in <code>courtvision/trackers.py</code> <pre><code>@staticmethod\ndef state_to_observation(\n    state: torch.Tensor,\n    *,\n    world_to_cam: torch.Tensor,\n    cam_to_image: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Map the state to the observation space.\n    This is from the 3D world space to the 2D image plane.\n\n    Args:\n        state (torch.Tensor): Tracker state. Shape: [N, state_dim]\n        world_to_cam (torch.Tensor): _description_\n        cam_to_image (torch.Tensor): _description_\n\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    x_y_z_1_positions = convert_points_to_homogeneous(\n        state[:, : StateIdx.z + 1].rename(None)\n    )\n    x_y_z_1_positions_cam = convert_points_from_homogeneous(\n        x_y_z_1_positions @ world_to_cam.T\n    )\n    return convert_points_from_homogeneous(x_y_z_1_positions_cam @ cam_to_image.T)\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.update","title":"<code>update(obs_state, score=torch.tensor(1.0))</code>","text":"<p>Update the state using the observation and it's associated score.</p> <p>Parameters:</p> Name Type Description Default <code>obs_state</code> <code>torch.tensor</code> <p>measurement in the image plane. Shape: [2]</p> required <code>score</code> <code>torch.tensor</code> <p>If the detector assigns a score this                             can be used in the update step.                             Defaults to torch.tensor(1.0).</p> <code>torch.tensor(1.0)</code> Source code in <code>courtvision/trackers.py</code> <pre><code>def update(self, obs_state: torch.tensor, score: torch.tensor = torch.tensor(1.0)):\n\"\"\"Update the state using the observation and it's associated score.\n\n    Args:\n        obs_state (torch.tensor): measurement in the image plane. Shape: [2]\n        score (torch.tensor, optional): If the detector assigns a score this\n                                        can be used in the update step.\n                                        Defaults to torch.tensor(1.0).\n    \"\"\"\n    likelihoods = self.likelihood(obs_state, self.states) * score\n    self.states[:, StateIdx.weight] = self.update_weights(\n        self.states[:, StateIdx.weight], likelihoods\n    )\n    self.states = self.resample(self.states, self.states[:, StateIdx.weight])\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.ParticleFilter.update_weights","title":"<code>update_weights(weights, likelihoods)</code>  <code>staticmethod</code>","text":"<p>Given the current weights and the likelihoods, update the weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>torch.tensor</code> <p>Nx1 tensor of weights</p> required <code>likelihoods</code> <code>torch.tensor</code> <p>Nx1 tensor of likelihoods</p> required <p>Returns:</p> Type Description <code>torch.tensor</code> <p>torch.tensor: Nx1 tensor of updated weights</p> Source code in <code>courtvision/trackers.py</code> <pre><code>@staticmethod\ndef update_weights(\n    weights: torch.tensor, likelihoods: torch.tensor\n) -&gt; torch.tensor:\n\"\"\"Given the current weights and the likelihoods, update the weights.\n\n    Args:\n        weights (torch.tensor): Nx1 tensor of weights\n        likelihoods (torch.tensor): Nx1 tensor of likelihoods\n\n    Returns:\n        torch.tensor: Nx1 tensor of updated weights\n    \"\"\"\n    return (\n        weights.rename(None)\n        * likelihoods\n        / (weights.rename(None) * likelihoods).sum()\n    )\n</code></pre>"},{"location":"api/trackers/#courtvision.trackers.StateIdx","title":"<code>StateIdx</code>","text":"<p>Named indices for the state tensor.</p> Source code in <code>courtvision/trackers.py</code> <pre><code>class StateIdx:\n\"\"\"\n    Named indices for the state tensor.\n    \"\"\"\n\n    x: int = 0\n    y: int = 1\n    z: int = 2\n    vx: int = 3\n    vy: int = 4\n    vz: int = 5\n    ax: int = 6\n    ay: int = 7\n    az: int = 8\n    weight: int = 9\n</code></pre>"},{"location":"api/trainer/","title":"courtvision.trainer","text":""},{"location":"api/trainer/#courtvision.trainer.log_wb_image_and_bbox","title":"<code>log_wb_image_and_bbox(images, preds, targets, logger, global_step)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>torch.tensor</code> <p>description</p> required <code>preds</code> <code>list[dict[str, torch.Tensor]]</code> <p>description</p> required <code>targets</code> <code>list[dict[str, torch.Tensor]]</code> <p>description</p> required <code>logger</code> <code>wandb.sdk.wandb_run.Run</code> <p>description</p> required <code>global_step</code> <code>int</code> <p>description</p> required Source code in <code>courtvision/trainer.py</code> <pre><code>def log_wb_image_and_bbox(\n    images: torch.tensor,\n    preds: list[dict[str, torch.Tensor]],\n    targets: list[dict[str, torch.Tensor]],\n    logger: wandb.sdk.wandb_run.Run,\n    global_step: int,\n):\n\"\"\"_summary_\n\n    Args:\n        images (torch.tensor): _description_\n        preds (list[dict[str, torch.Tensor]]): _description_\n        targets (list[dict[str, torch.Tensor]]): _description_\n        logger (wandb.sdk.wandb_run.Run): _description_\n        global_step (int): _description_\n    \"\"\"\n    images_to_log = []\n    for image, pred, target in zip(images, preds, targets):\n        image_height, image_width = image.shape[-2:]\n        images_to_log.append(\n            wandb.Image(\n                image.permute(1, 2, 0).cpu().numpy(),\n                boxes={\n                    \"predictions\": {\n                        \"box_data\": [\n                            {\n                                \"position\": {\n                                    \"minX\": float(x_min) / image_width,\n                                    \"maxX\": float(x_max) / image_width,\n                                    \"minY\": float(y_min) / image_height,\n                                    \"maxY\": float(y_max) / image_height,\n                                },\n                                \"class_id\": 1,\n                                \"box_caption\": \"ball\",\n                                \"scores\": {\"score\": float(score)},\n                            }\n                            for (x_min, y_min, x_max, y_max), score in zip(\n                                pred[\"boxes\"].cpu().numpy(),\n                                pred[\"scores\"].cpu().numpy(),\n                                strict=True,\n                            )\n                        ]\n                    },\n                    \"targets\": {\n                        \"box_data\": [\n                            {\n                                \"position\": {\n                                    \"minX\": float(x_min) / image_width,\n                                    \"maxX\": float(x_max) / image_width,\n                                    \"minY\": float(y_min) / image_height,\n                                    \"maxY\": float(y_max) / image_height,\n                                },\n                                \"class_id\": 1,\n                                \"box_caption\": \"ball\",\n                            }\n                            for x_min, y_min, x_max, y_max in target[\"boxes\"]\n                            .cpu()\n                            .numpy()\n                        ],\n                    },\n                },\n            )\n        )\n\n    logger.log({\"image\": images_to_log})\n</code></pre>"},{"location":"api/vision/","title":"courtvision.vis","text":""},{"location":"api/vision/#courtvision.vis.load_image","title":"<code>load_image(file_name)</code>","text":"<p>Loads the image with OpenCV.</p> Source code in <code>courtvision/vis.py</code> <pre><code>def load_image(file_name):\n\"\"\"Loads the image with OpenCV.\"\"\"\n    assert os.path.isfile(file_name), f\"Invalid file {file_name}\"  # nosec\n    # load image with OpenCV\n    if isinstance(file_name, Path):\n        file_name = file_name.as_posix()\n    return cv2.cvtColor(cv2.imread(file_name, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n</code></pre>"},{"location":"api/vision/#courtvision.vis.load_timg","title":"<code>load_timg(file_name)</code>","text":"<p>Loads the image with OpenCV and converts to torch.Tensor.</p> Source code in <code>courtvision/vis.py</code> <pre><code>def load_timg(file_name):\n\"\"\"Loads the image with OpenCV and converts to torch.Tensor.\"\"\"\n    img = load_image(file_name)\n    # convert image to torch tensor\n    return K.image_to_tensor(img, None).float() / 255.0\n</code></pre>"},{"location":"api/vision/#courtvision.vis.log_court_layout","title":"<code>log_court_layout(camera_matrix, image_width, image_height, translation_vector, rotation_vector, court_mesh_path)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>camera_matrix</code> <code>np.array</code> <p>description</p> required <code>image_width</code> <code>int</code> <p>description</p> required <code>image_height</code> <code>int</code> <p>description</p> required <code>translation_vector</code> <code>np.array</code> <p>description</p> required <code>rotation_vector</code> <code>np.array</code> <p>description</p> required <code>court_mesh_path</code> <code>Path</code> <p>description</p> required Source code in <code>courtvision/vis.py</code> <pre><code>def log_court_layout(\n    camera_matrix: np.array,\n    image_width: int,\n    image_height: int,\n    translation_vector: np.array,\n    rotation_vector: np.array,\n    court_mesh_path: Path,\n):\n\"\"\"_summary_\n\n    Args:\n        camera_matrix (np.array): _description_\n        image_width (int): _description_\n        image_height (int): _description_\n        translation_vector (np.array): _description_\n        rotation_vector (np.array): _description_\n        court_mesh_path (Path): _description_\n    \"\"\"\n    rr.log_pinhole(\n        \"world/camera/image\",\n        child_from_parent=camera_matrix,\n        width=image_width,\n        height=image_height,\n        timeless=True,\n    )\n    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n    rr.log_transform3d(\n        \"world/camera\",\n        transform=rr.TranslationAndMat3(\n            translation=translation_vector.squeeze(),\n            matrix=rotation_matrix,\n        ),\n        from_parent=True,\n    )\n    rr.log_point(\"world/front_left\", np.array([0, 0, 0]))\n    # TODO: this should be refectored to use the court_size\n    rr.log_point(\"world/front_right\", np.array([100, 0, 0]))\n    rr.log_mesh_file(\n        \"world\",\n        mesh_format=rr.MeshFormat(\"GLB\"),\n        mesh_path=court_mesh_path,\n    )\n</code></pre>"},{"location":"api/vision/#courtvision.vis.plot_3d_lines","title":"<code>plot_3d_lines(xs, ys, zs, plt_axis=None, view_init=(90.0, 0.0, 0.0))</code>","text":"<p>plots lines on a Axes3D</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>np.array</code> <p>array of x start and stop coordinates</p> required <code>ys</code> <code>np.array</code> <p>array of y start and stop coordinates</p> required <code>zs</code> <code>np.array</code> <p>array of z start and stop coordinates</p> required <code>plt_axis</code> <code>None | Axes3D</code> <p>Axes3D to draw on. If not given one will be created Defaults to None.</p> <code>None</code> <code>view_init</code> <code>tuple[float, float, float]</code> <p>Position of the camera to view. Defaults to (90., 0., 0.).</p> <code>(90.0, 0.0, 0.0)</code> <p>Returns:</p> Name Type Description <code>Axes3D</code> <code>Axes3D</code> <p>matplotlib ax that can be added to or shown.</p> Source code in <code>courtvision/vis.py</code> <pre><code>def plot_3d_lines(\n    xs: np.array,\n    ys: np.array,\n    zs: np.array,\n    plt_axis: None | Axes3D = None,\n    view_init: tuple[float, float, float] = (90.0, 0.0, 0.0),\n) -&gt; Axes3D:\n\"\"\"plots lines on a Axes3D\n\n    Args:\n        xs (np.array): array of x start and stop coordinates\n        ys (np.array): array of y start and stop coordinates\n        zs (np.array): array of z start and stop coordinates\n        plt_axis (None | Axes3D, optional): Axes3D to draw on. If not given one will be created Defaults to None.\n        view_init (tuple[float, float, float], optional): Position of the camera to view. Defaults to (90., 0., 0.).\n\n    Returns:\n        Axes3D: matplotlib ax that can be added to or shown.\n    \"\"\"\n\n    fig = None\n    if plt_axis is None:\n        fig = plt.figure()\n        plt_axis = fig.add_subplot(111, projection=\"3d\")\n    for i in range(len(xs)):\n        plt_axis.plot(xs[i], ys[i], zs[i], c=\"b\")\n    # set the axis labels\n    plt_axis.set_xlabel(\"X\")\n    plt_axis.set_ylabel(\"Y\")\n    plt_axis.set_zlabel(\"Z\")\n    plt_axis.set_aspect(\"equal\")\n    plt_axis.margins(x=0)\n    plt_axis.view_init(*view_init)\n    return plt_axis, fig\n</code></pre>"},{"location":"api/vision/#courtvision.vis.plot_coords","title":"<code>plot_coords(img, src_coords, show=True, thickness=2)</code>","text":"<p>Draws lines on 'img'.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>np.array</code> <p>description</p> required <code>src_coords</code> <code>np.array</code> <p>description</p> required <code>show</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> Source code in <code>courtvision/vis.py</code> <pre><code>def plot_coords(img: np.array, src_coords: np.array, show: bool = True, thickness=2):\n\"\"\"Draws lines on 'img'.\n\n    Args:\n        img (np.array): _description_\n        src_coords (np.array): _description_\n        show (bool, optional): _description_. Defaults to True.\n    \"\"\"\n    src_coords = src_coords.astype(int)\n    cv2.polylines(img, [src_coords], True, (0, 255, 0), thickness=2)\n    if show:\n        from matplotlib import pyplot as plt\n\n        plt.imshow(img)\n</code></pre>"},{"location":"api/vision/#courtvision.vis.plot_n_images_in_a_grid","title":"<code>plot_n_images_in_a_grid(images, n_cols=3)</code>","text":"<p>draws a grid of images</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[np.array]</code> <p>images to draw on - this is inplace</p> required <code>n_cols</code> <code>int</code> <p>number of cols. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>fig, ax</code> <p>matplotlib fig and ax</p> Source code in <code>courtvision/vis.py</code> <pre><code>def plot_n_images_in_a_grid(images: list[np.array], n_cols: int = 3):\n\"\"\"draws a grid of images\n\n    Args:\n        images (list[np.array]): images to draw on - this is inplace\n        n_cols (int, optional): number of cols. Defaults to 3.\n\n    Returns:\n        tuple(fig, ax): matplotlib fig and ax\n    \"\"\"\n    n_cols = min(n_cols, len(images))\n    n_rows = int(np.ceil(len(images) / n_cols))\n    fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n    if n_rows == 1:\n        ax = ax.reshape(1, n_cols)\n    for i, img in enumerate(images):\n        ax[i // n_cols, i % n_cols].imshow(img)\n    return fig, ax\n</code></pre>"}]}