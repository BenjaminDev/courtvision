{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    Field,\n",
    "    annotated_types,\n",
    "    parse_obj_as,\n",
    "    validator,\n",
    "    root_validator,\n",
    ")\n",
    "from typing import Annotated\n",
    "import torchvision\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FastRCNNPredictor,\n",
    "    fasterrcnn_resnet50_fpn,\n",
    ")\n",
    "\n",
    "\n",
    "# dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from courtvision.data import BallDataSet\n",
    "\n",
    "with open(\n",
    "    \"/Users/benjamindecharmoy/projects/courtvision/datasets/annotations/project-1-at-2023-04-22-11-17-eb4e8e32.json\",\n",
    "    \"r\",\n",
    ") as fp:\n",
    "    dataset = BallDataSet(samples=json.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnotationDataPath(image=PosixPath('/data/upload/1/83dcc83f-curated_001_frame_0001.png'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.samples[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from torchvision.datasets import VisionDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from courtvision.vis import load_image\n",
    "\n",
    "\n",
    "class BallDataset(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: BallDataSet,\n",
    "        root: str,\n",
    "        transforms: Callable | None = None,\n",
    "        transform: Callable | None = None,\n",
    "        target_transform: Callable | None = None,\n",
    "    ):\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.dataset.annotations[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def show_sample(sample: CourtAnnotatedSample):\n",
    "        \"\"\"Plots an image and its annotation\"\"\"\n",
    "        image = load_image(sample.data.image)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.scatter(\n",
    "            sample[\"annotation\"][:, 0],\n",
    "            sample[\"annotation\"][:, 1],\n",
    "            marker=\"x\",\n",
    "            s=100,\n",
    "            c=\"r\",\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = FastRCNNPredictor(384, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NestedTensorBlock(\n",
       "   (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (attn): MemEffAttention(\n",
       "     (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "     (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "     (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "     (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls1): LayerScale()\n",
       "   (drop_path1): Identity()\n",
       "   (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (mlp): Mlp(\n",
       "     (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (act): GELU(approximate='none')\n",
       "     (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (drop): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (ls2): LayerScale()\n",
       "   (drop_path2): Identity()\n",
       " )]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed, mod_list = list(dinov2_vits14.children())[:-2]\n",
    "patch_embed, mod_list\n",
    "\n",
    "list(mod_list.children())[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x98304 and 384x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m head(nn\u001b[39m.\u001b[39;49mSequential(\u001b[39m*\u001b[39;49m[patch_embed, \u001b[39m*\u001b[39;49mmod_list[:]])(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m)))\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/projects/courtvision/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/courtvision/.venv/lib/python3.11/site-packages/torchvision/models/detection/faster_rcnn.py:366\u001b[0m, in \u001b[0;36mFastRCNNPredictor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    361\u001b[0m     torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m    362\u001b[0m         \u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]) \u001b[39m==\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m    363\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx has the wrong shape, expecting the last two dimensions to be [1,1] instead of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    365\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls_score(x)\n\u001b[1;32m    367\u001b[0m bbox_deltas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_pred(x)\n\u001b[1;32m    369\u001b[0m \u001b[39mreturn\u001b[39;00m scores, bbox_deltas\n",
      "File \u001b[0;32m~/projects/courtvision/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/courtvision/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x98304 and 384x2)"
     ]
    }
   ],
   "source": [
    "head(nn.Sequential(*[patch_embed, *mod_list[:]])(torch.rand(1, 3, 224, 224))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindecharmoy/projects/courtvision/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/benjamindecharmoy/projects/courtvision/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model = fasterrcnn_resnet50_fpn(\n",
    "    pretrained=True,\n",
    "    num_classes=91,\n",
    ")\n",
    "fast_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from courtvision.vis import load_timg\n",
    "\n",
    "image = load_timg(\n",
    "    \"/Users/benjamindecharmoy/projects/courtvision/data/frames/curated_001/frame_0001.png\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    # fast_model.backbone.body = nn.Sequential(*[patch_embed, *mod_list[:]])\n",
    "    # fast_model.roi_heads.box_predictor = head\n",
    "    fast_model.eval()\n",
    "    out = fast_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # fast_model.backbone.body = nn.Sequential(*[patch_embed, *mod_list[:]])\n",
    "    # fast_model.roi_heads.box_predictor = head\n",
    "    model.eval()\n",
    "    out = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "from rerun.components.rect2d import RectFormat\n",
    "\n",
    "rr.init(\"faster\", spawn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.log_rigid3(entity_path=\"frame/test\", parent_from_child=([0, 10, 0], [0, 111, 0, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.log_image(\"frame/test/img\", image.squeeze(0).permute(1, 2, 0))\n",
    "# out[0][\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = [box.tolist() for box in out[0][\"boxes\"]]\n",
    "rr.log_rects(\"frame/boxes\", dd, rect_format=RectFormat.XYXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rerun.components.rect2d import RectFormat\n",
    "\n",
    "rr.log_rect(\"frame/box\", out[0][\"boxes\"][2].tolist(), rect_format=RectFormat.XYXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[586.6439819335938, 360.8269348144531, 1062.5076904296875, 670.1864624023438]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][\"boxes\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 720, 1280])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[ 586.6440,  360.8269, 1062.5077,  670.1865],\n",
       "         [ 597.6040,  191.3133,  617.9384,  211.2779],\n",
       "         [ 297.4283,  581.7656, 1115.0757,  708.7494],\n",
       "         [ 389.4426,  128.8055,  970.5952,  490.9042],\n",
       "         [ 434.5761,  284.2084,  946.3016,  621.3353],\n",
       "         [ 761.4454,  195.3744,  802.9850,  293.6284],\n",
       "         [ 428.8613,  451.5882,  473.8964,  573.3542],\n",
       "         [ 821.0404,  284.2837, 1154.2217,  720.0000],\n",
       "         [ 898.5524,  426.9127,  946.2028,  537.3666],\n",
       "         [ 416.9436,  498.2014, 1103.0070,  664.2693],\n",
       "         [ 878.7841,  415.2511,  968.8537,  574.6574],\n",
       "         [ 757.9148,   17.3750,  977.1315,  369.8183],\n",
       "         [ 763.1102,  232.0803,  817.0454,  287.9791],\n",
       "         [ 573.9623,  174.1141,  595.5928,  222.5417],\n",
       "         [ 588.5248,  189.3533,  609.8690,  209.1296],\n",
       "         [ 406.5399,   64.9292,  815.7542,  349.9398],\n",
       "         [ 471.6840,   19.7566,  897.4198,  720.0000],\n",
       "         [1229.1489,  222.3493, 1280.0000,  271.0620],\n",
       "         [1206.4070,  206.4960, 1265.9695,  281.6807],\n",
       "         [ 938.0323,  244.5717, 1164.2694,  638.3610],\n",
       "         [ 580.3422,  186.7864,  599.3096,  212.4561],\n",
       "         [ 265.7435,  318.4493,  740.8693,  645.5605],\n",
       "         [ 417.4731,  476.3711,  460.0914,  585.4021],\n",
       "         [ 689.3873,  243.0820, 1179.2812,  600.9705],\n",
       "         [ 773.9100,  250.0592,  791.6722,  280.6608],\n",
       "         [ 738.3953,   82.2972,  761.3566,  118.5267],\n",
       "         [ 674.9299,   32.7672, 1078.0176,  720.0000],\n",
       "         [ 686.2373,   74.7116,  707.0234,  124.8422],\n",
       "         [ 200.3016,   22.8571,  651.6734,  327.7759],\n",
       "         [ 771.9380,  227.8901,  811.8510,  263.9630],\n",
       "         [ 706.6463,   75.4656,  755.5440,  122.5879],\n",
       "         [ 140.5085,  500.3714,  849.5234,  658.9361],\n",
       "         [ 765.7311,  216.9080,  802.6503,  270.5646],\n",
       "         [ 774.1589,  202.1648,  816.9133,  315.2592],\n",
       "         [ 309.3848,   34.7362,  704.6791,  720.0000],\n",
       "         [1044.1138,   37.5113, 1061.9276,   68.5182],\n",
       "         [1203.5609,  181.1958, 1280.0000,  299.4287],\n",
       "         [ 554.7537,  219.3676,  592.2333,  237.5180],\n",
       "         [ 529.8947,   87.2754,  561.6721,  123.2998],\n",
       "         [ 769.8409,  244.0891,  788.2586,  275.8706],\n",
       "         [ 640.1352,   15.9172, 1096.2388,  306.3579],\n",
       "         [ 357.5866,  147.9003, 1034.4116,  315.2721],\n",
       "         [ 212.1252,  376.8621,  896.9985,  576.7485],\n",
       "         [ 548.5900,  177.6436,  576.0354,  226.9788],\n",
       "         [ 371.7676,   51.8902, 1038.5177,  191.0862],\n",
       "         [ 594.9320,  185.6950,  614.5525,  202.6071],\n",
       "         [ 535.2789,  187.3527,  583.4839,  230.0842],\n",
       "         [ 275.1154,  232.1993, 1062.3204,  720.0000],\n",
       "         [1184.4305,  187.7363, 1238.1620,  276.7061],\n",
       "         [ 578.0175,   58.8350,  592.0490,   82.5920],\n",
       "         [ 599.7522,  187.3485,  612.0826,  200.4277],\n",
       "         [ 403.8696,  432.8505,  459.2826,  569.1490],\n",
       "         [ 584.0450,  176.3823,  608.3876,  218.1958],\n",
       "         [ 779.8032,  268.9927,  985.7401,  349.3292],\n",
       "         [ 579.3033,   64.7774,  596.4821,   86.9004],\n",
       "         [ 775.5151,  239.2881,  799.3024,  276.5553],\n",
       "         [ 520.9483,   91.5498,  541.1411,  113.9511],\n",
       "         [ 313.6845,  633.8190, 1280.0000,  718.8735],\n",
       "         [1196.6780,  180.2011, 1241.1735,  232.9879],\n",
       "         [1107.2968,   27.6635, 1132.9215,   77.5457],\n",
       "         [ 820.5093,  385.4928, 1067.7268,  627.3840],\n",
       "         [ 393.3371,  458.3569,  485.2750,  581.6778],\n",
       "         [1168.6537,  175.4663, 1280.0000,  720.0000],\n",
       "         [ 348.8717,  427.6295,  984.9733,  619.1979],\n",
       "         [  70.6365,  225.5276,  802.1204,  416.7345],\n",
       "         [ 270.1734,   90.9126,  713.4437,  423.4013],\n",
       "         [ 179.6345,   80.3418,  568.3032,  720.0000],\n",
       "         [ 772.5125,   68.9942,  796.3948,   88.7850],\n",
       "         [ 752.3193,  193.6955,  836.5873,  305.7368],\n",
       "         [ 481.2711,  375.4101, 1193.2704,  557.9867],\n",
       "         [ 375.3463,   59.0375,  421.7011,  116.5521],\n",
       "         [1179.8378,   92.5516, 1280.0000,  235.0270],\n",
       "         [ 532.4174,  214.1263, 1243.7896,  377.0830],\n",
       "         [ 624.5856,   96.2770,  651.5331,  123.5502],\n",
       "         [   3.7085,  281.0132,  809.7508,  717.8148],\n",
       "         [ 503.0773,   82.4684,  527.4305,  119.0936],\n",
       "         [1237.3408,  229.9048, 1279.9969,  308.3493],\n",
       "         [ 968.2909,   62.6300,  987.0865,   95.0889],\n",
       "         [ 697.5464,   57.1963,  736.9855,  125.2095],\n",
       "         [ 476.7883,   72.0956,  491.4384,  105.4976],\n",
       "         [ 704.4578,   83.9219,  725.2395,  113.2113],\n",
       "         [ 541.2117,   17.4296, 1161.7324,  124.4079],\n",
       "         [1219.0945,  178.0065, 1271.5823,  229.5258],\n",
       "         [ 741.1360,   71.8149,  783.4855,  119.6490],\n",
       "         [ 435.0513,  189.4568,  808.4263,  239.2385],\n",
       "         [ 677.4348,   84.5562,  761.2742,  120.9649],\n",
       "         [ 507.4766,   81.9820,  551.2498,  123.3125],\n",
       "         [1260.9211,  222.0314, 1278.8541,  249.3578],\n",
       "         [ 777.3678,   66.9311,  798.9340,  101.6974],\n",
       "         [ 778.7320,   63.7131,  814.6528,  121.4260],\n",
       "         [ 800.6712,   87.0410,  815.2926,  101.4947],\n",
       "         [ 433.6327,   67.5399,  479.7768,  125.8416],\n",
       "         [1123.4772,  196.2231, 1186.6086,  242.8383],\n",
       "         [ 487.6898,   92.0726,  502.9479,  117.7427],\n",
       "         [ 587.6027,   55.0480,  601.0876,   82.6571],\n",
       "         [ 256.5641,  210.4545,  986.2704,  367.0491],\n",
       "         [ 397.8264,  482.2935,  419.5699,  509.2046],\n",
       "         [ 542.0276,  184.2871,  557.4905,  205.9917],\n",
       "         [ 540.4183,   87.5220,  559.2673,  114.9173],\n",
       "         [ 666.0954,  208.9873,  906.8708,  291.7126]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'scores': tensor([0.7888, 0.7787, 0.7760, 0.7714, 0.7642, 0.7639, 0.7626, 0.7608, 0.7583, 0.7578, 0.7519, 0.7496, 0.7455, 0.7393, 0.7371, 0.7357, 0.7307, 0.7282, 0.7271, 0.7266, 0.7262, 0.7262, 0.7242, 0.7232, 0.7226, 0.7195, 0.7195, 0.7187, 0.7183, 0.7180, 0.7159, 0.7141, 0.7137, 0.7105, 0.7105, 0.7103, 0.7081, 0.7055, 0.7035,\n",
       "         0.7034, 0.7002, 0.6985, 0.6976, 0.6969, 0.6969, 0.6945, 0.6942, 0.6942, 0.6929, 0.6911, 0.6891, 0.6877, 0.6876, 0.6874, 0.6852, 0.6848, 0.6820, 0.6819, 0.6815, 0.6812, 0.6811, 0.6808, 0.6802, 0.6795, 0.6791, 0.6783, 0.6761, 0.6753, 0.6742, 0.6742, 0.6740, 0.6696, 0.6685, 0.6681, 0.6680, 0.6678, 0.6667, 0.6656,\n",
       "         0.6652, 0.6645, 0.6635, 0.6631, 0.6628, 0.6625, 0.6614, 0.6613, 0.6613, 0.6612, 0.6612, 0.6607, 0.6591, 0.6589, 0.6583, 0.6575, 0.6572, 0.6562, 0.6561, 0.6559, 0.6559, 0.6546])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'label_studio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlabel_studio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'label_studio'"
     ]
    }
   ],
   "source": [
    "import label_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
