{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from courtvision.trackers import Tracker, StateIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from torch.testing import assert_close\n",
    "\n",
    "tracker = Tracker(num_particles=1000)\n",
    "assert all(tracker.states[:, StateIdx.weight] > 0.0)\n",
    "tracker.states[:, 9] = Tracker.normalized_weights(states=tracker.states)\n",
    "\n",
    "assert_close(tracker.states.select(\"state\", 9).sum().item(), 1.0, atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set state\n",
    "import torch\n",
    "\n",
    "tracker = Tracker(num_particles=1000)\n",
    "tracker.set_states_to(torch.tensor([1.0, 2.0, 3.0]))\n",
    "assert_close(tracker.states[:, StateIdx.x].mean().item(), 1.0, atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss\n",
    "import torch\n",
    "\n",
    "# Test:\n",
    "tracker = Tracker(num_particles=1000)\n",
    "obs_in_image = torch.tensor([[1.0, 1.0]]).repeat(1000, 1)\n",
    "\n",
    "# tracker.normalize_weights()\n",
    "tracker.H = torch.eye(3, 4)\n",
    "# assert (tracker.states[:,:3] - Tracker.state_to_observation(tracker.states, tracker.H)).sum() == 0.0\n",
    "assert Tracker.state_to_observation(tracker.states, tracker.H).shape == torch.Size(\n",
    "    [1000, 2]\n",
    ")\n",
    "pred_image_states = Tracker.state_to_observation(tracker.states, tracker.H)\n",
    "likelihoods = Tracker.likelihood(obs_state=obs_in_image, pred_state=pred_image_states)\n",
    "assert all(tracker.states.select(\"state\", 9) > 0.0), \"weights should be positive\"\n",
    "\n",
    "assert all(likelihoods > 0.0), \"likelihoods should be positive\"\n",
    "tracker.states[:, 9] = Tracker.update_weights(\n",
    "    tracker.states.select(\"state\", 9), likelihoods\n",
    ")\n",
    "assert all(tracker.states.select(\"state\", 9) > 0.0), \"weights should be positive\"\n",
    "tracker.states = Tracker.resample(\n",
    "    tracker.states.rename(None), tracker.states.select(\"state\", 9)\n",
    ")\n",
    "# print(likelihoods.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3D point in homogeneous coordinates\n",
    "# P_hom = np.array([[X], [Y], [Z], [1]], dtype=np.float32)\n",
    "\n",
    "# # Project the 3D point to the image plane using the projection matrix\n",
    "# P_proj_hom = np.dot(P, P_hom)\n",
    "\n",
    "# # Normalize the image coordinates\n",
    "# P_proj = P_proj_hom[:2] / P_proj_hom[2]\n",
    "\n",
    "# # Print the projected point on the image plane\n",
    "# print(\"Projected point:\", P_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['a_front_left', 'b_front_right', 'c_back_left', 'd_back_right', 'e_left_near_serve_line', 'f_right_near_serve_line', 'g_left_far_serve_line', 'h_right_far_serve_line', 'm_top_front_left', 'n_top_front_right', 'o_top_back_left', 'p_top_back_right']),\n",
       " dict_keys(['a_front_left', 'b_front_right', 'c_back_left', 'd_back_right', 'e_left_near_serve_line', 'f_right_near_serve_line', 'g_left_far_serve_line', 'h_right_far_serve_line', 'm_top_front_left', 'n_top_front_right', 'o_top_back_left', 'p_top_back_right']))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from courtvision.geometry import corners_world_3d, convert_corners_to_coords\n",
    "\n",
    "convert_corners_to_coords(corners_world_3d)\n",
    "from courtvision.geometry import get_corners_image, get_corners_verital_plane_on_image\n",
    "\n",
    "corners_image = get_corners_image(\"curated_001/curated_001_frame_0001\")\n",
    "corners_image_other = get_corners_verital_plane_on_image(\n",
    "    \"curated_001/curated_001_frame_0001\", plane=\"both\"\n",
    ")\n",
    "corners_image.update(corners_image_other)\n",
    "for k in list(corners_world_3d.keys()):\n",
    "    if k not in corners_image:\n",
    "        corners_world_3d.pop(k)\n",
    "\n",
    "\n",
    "corners_image.keys(), corners_world_3d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_points=tensor([[[0.1111, 0.8790],\n",
      "         [0.8944, 0.8864],\n",
      "         [0.3119, 0.2900],\n",
      "         [0.6865, 0.2894],\n",
      "         [0.1661, 0.7259],\n",
      "         [0.8415, 0.7278],\n",
      "         [0.2964, 0.3436],\n",
      "         [0.7097, 0.3439],\n",
      "         [0.0878, 0.4894],\n",
      "         [0.9141, 0.4885],\n",
      "         [0.3067, 0.0870],\n",
      "         [0.6949, 0.6949]]], dtype=torch.float64)\n",
      "img_points_d=tensor([[[0.1111, 0.8790],\n",
      "         [0.8944, 0.8864],\n",
      "         [0.3119, 0.2900],\n",
      "         [0.6865, 0.2894],\n",
      "         [0.1661, 0.7259],\n",
      "         [0.8415, 0.7278],\n",
      "         [0.2964, 0.3436],\n",
      "         [0.7097, 0.3439],\n",
      "         [0.0878, 0.4894],\n",
      "         [0.9141, 0.4885],\n",
      "         [0.3067, 0.0870],\n",
      "         [0.6949, 0.6949]]], dtype=torch.float64)\n",
      "torch.Size([1, 12, 3]) torch.Size([1, 12, 2]) torch.Size([1, 3, 3])\n",
      "success=True rvec=array([[ 1.73129197],\n",
      "       [-0.4755916 ],\n",
      "       [ 1.13913989]]) tvec=array([[-1939726.92634732],\n",
      "       [-1090176.10143233],\n",
      "       [ 2426190.30146557]])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
       "         [ 5.0025e-04, -1.0000e+00, -1.0000e+00],\n",
       "         [-1.0000e+00,  3.0040e+00, -1.0000e+00],\n",
       "         [ 5.0025e-04,  3.0040e+00, -1.0000e+00],\n",
       "         [-1.0000e+00, -5.9960e-01, -1.0000e+00],\n",
       "         [ 5.0025e-04, -5.9960e-01, -1.0000e+00],\n",
       "         [-1.0000e+00,  2.6036e+00, -1.0000e+00],\n",
       "         [ 5.0025e-04,  2.6036e+00, -1.0000e+00],\n",
       "         [-1.0000e+00, -1.0000e+00,  1.0067e+00],\n",
       "         [ 5.0025e-04, -1.0000e+00,  1.0067e+00],\n",
       "         [-1.0000e+00,  3.0040e+00,  1.0067e+00],\n",
       "         [ 5.0025e-04,  3.0040e+00,  1.0067e+00]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from courtvision.geometry import corners_world_3d\n",
    "\n",
    "#  REF https://ksimek.github.io/2013/08/13/intrinsic/\n",
    "world_points = torch.tensor(\n",
    "    convert_corners_to_coords(corners_world_3d), dtype=torch.float64\n",
    ").unsqueeze(0)\n",
    "\n",
    "img_points = torch.tensor(\n",
    "    convert_corners_to_coords(corners_image), dtype=torch.float64\n",
    ").unsqueeze(0)\n",
    "print(f\"{img_points=}\")\n",
    "from kornia.geometry.conversions import (\n",
    "    normalize_pixel_coordinates,\n",
    "    denormalize_pixel_coordinates,\n",
    "    normalize_pixel_coordinates3d,\n",
    ")\n",
    "\n",
    "img_points_N = normalize_pixel_coordinates(img_points, width=1280, height=720)\n",
    "img_points_d = denormalize_pixel_coordinates(img_points_N, width=1280, height=720)\n",
    "print(f\"{img_points_d=}\")\n",
    "fx = 800\n",
    "fy = 800\n",
    "cy = 720 / 2\n",
    "\n",
    "cx = 1280 / 2\n",
    "\n",
    "\n",
    "intrinsics = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [fx, 0.0, cx],\n",
    "            [0.0, fy, cy],\n",
    "            [0.0, 0.0, 1.0],\n",
    "        ]\n",
    "    ],\n",
    "    dtype=torch.float64,\n",
    ")\n",
    "\n",
    "print(world_points.shape, img_points.shape, intrinsics.shape)\n",
    "# torch.Size([1, 6, 3]) torch.Size([1, 6, 2]) torch.Size([1, 3, 3])\n",
    "world_points_N = normalize_pixel_coordinates3d(\n",
    "    world_points, depth=2000.0, height=300.0, width=1000.0\n",
    ")\n",
    "# Solve PnP\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "dist_coeffs = np.array([0, 0, 0, 0, 0], dtype=np.float32)\n",
    "\n",
    "success, rvec, tvec = cv2.solvePnP(\n",
    "    world_points_N.numpy(),\n",
    "    img_points_N.numpy(),\n",
    "    intrinsics.squeeze(0).numpy(),\n",
    "    dist_coeffs,\n",
    "    flags=cv2.SOLVEPNP_UPNP,\n",
    ")\n",
    "print(f\"{success=} {rvec=} {tvec=}\")\n",
    "pred_world_to_cam = kornia.geometry.solve_pnp_dlt(\n",
    "    world_points_N, img_points_N, intrinsics, svd_eps=0.0000000001\n",
    ")\n",
    "\n",
    "print(pred_world_to_cam.shape)\n",
    "# torch.Size([1, 3, 4])\n",
    "\n",
    "pred_world_to_cam\n",
    "world_points_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intrinsics.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1939726.92634732],\n",
       "        [-1090176.10143233],\n",
       "        [ 2426190.30146557]]),\n",
       " array([[ 1.73129197],\n",
       "        [-0.4755916 ],\n",
       "        [ 1.13913989]]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec, rvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_world_to_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker(num_particles=1000, world_to_cam=pred_world_to_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8979e-01,  7.1178e-01, -3.8146e-01,  9.6423e+02],\n",
       "         [-3.3216e-01,  2.1675e-01,  9.1798e-01,  5.4303e+02],\n",
       "         [ 7.3609e-01,  6.6812e-01,  1.0859e-01, -1.2034e+03]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker.H.to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8005, -0.4500],\n",
       "         [-0.8005, -0.4500],\n",
       "         [-0.8005, -0.4500],\n",
       "         ...,\n",
       "         [-0.8005, -0.4500],\n",
       "         [-0.8005, -0.4500],\n",
       "         [-0.8005, -0.4500]]),\n",
       " tensor([[[-0.9998, -0.9976],\n",
       "          [-0.9986, -0.9975],\n",
       "          [-0.9995, -0.9992],\n",
       "          [-0.9989, -0.9992],\n",
       "          [-0.9997, -0.9980],\n",
       "          [-0.9987, -0.9980],\n",
       "          [-0.9995, -0.9990],\n",
       "          [-0.9989, -0.9990],\n",
       "          [-0.9999, -0.9986],\n",
       "          [-0.9986, -0.9986],\n",
       "          [-0.9995, -0.9998],\n",
       "          [-0.9989, -0.9981]]], dtype=torch.float64))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_in_image = torch.tensor([[1.0, 1.0]]).repeat(1000, 1)\n",
    "[0.8944, 0.8864],\n",
    "from kornia.geometry.conversions import normalize_homography3d\n",
    "\n",
    "tracker.set_states_to(torch.tensor([-1.0, -1.0, -1.0]))\n",
    "dd = Tracker.state_to_observation(\n",
    "    tracker.states, tracker.H.to(dtype=torch.float32).squeeze(0)\n",
    ")\n",
    "dd, img_points_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8012, -0.4512],\n",
       "        [-0.8012, -0.4512],\n",
       "        [-0.8012, -0.4512],\n",
       "        ...,\n",
       "        [-0.8012, -0.4512],\n",
       "        [-0.8012, -0.4512],\n",
       "        [-0.8012, -0.4512]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0870, dtype=torch.float64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_points.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success=True rvec=array([[0.],\n",
      "       [0.],\n",
      "       [0.]]) tvec=array([[-31.75634152],\n",
      "       [-23.55795647],\n",
      "       [ 51.23990293]])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 3D object points in world coordinates\n",
    "object_points = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0]], dtype=np.float32)\n",
    "\n",
    "# 2D image points in image coordinates\n",
    "image_points = np.array([[10, 10], [20, 10], [10, 20], [20, 20]], dtype=np.float32)\n",
    "\n",
    "# Camera intrinsic matrix (fx, fy, cx, cy)\n",
    "camera_matrix = np.array([[500, 0, 320], [0, 500, 240], [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# Distortion coefficients (k1, k2, p1, p2, k3)\n",
    "dist_coeffs = np.array([0, 0, 0, 0, 0], dtype=np.float32)\n",
    "\n",
    "# Solve PnP\n",
    "success, rvec, tvec = cv2.solvePnP(\n",
    "    object_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_UPNP\n",
    ")\n",
    "print(f\"{success=} {rvec=} {tvec=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
